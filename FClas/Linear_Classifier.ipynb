{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMIN339 : Méthodes Avncées de la Science de données\n",
    "\n",
    "\n",
    "## **`Réconnaissance Visuelle de Plantes`**\n",
    "\n",
    "### Object du Project :\n",
    "Réconnaissance d'espèces de plantes à partir de photos\n",
    "\n",
    "### Jeu de Départ : \n",
    "3474 images appartenant à 50 espèces différentes\n",
    "\n",
    "### Encadrement :\n",
    "* **`Konstantin TODOROV`**\n",
    "* **`Pascal PONCELET`**\n",
    " \n",
    "### Fait par :\n",
    "* **`BEYA NTUMBA Joel`**\n",
    "* **`MINKO AMOA Dareine`**\n",
    "* **`QUENETTE Christophe`**\n",
    "* **`SHAQURA Tasnim`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifcation of the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import cv2 as cv\n",
    "import collections\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed images trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-be89f7323a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Sort the dictionaries so it matches the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "%store -r preprocessed_images\n",
    "X, y = preprocessed_images\n",
    "\n",
    "# Sort the dictionaries so it matches the labels\n",
    "y = list(y.values())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "# testsize_train= 1-validation_size\n",
    "# testsize = 1 - validation_size\n",
    "seed=42\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. StandardScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a4949b121864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_test = sc.transform(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupenv/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupenv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupenv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    667\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    668\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupenv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 570\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. StandardScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "# Feature Scaling to bring the variable in a single scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de la variable à prédire via OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2084, 100, 100, 3)\n",
      "X_test: (209, 100, 100, 3)\n",
      "X_val: (417, 100, 100, 3)\n",
      "y_train: (2084, 2)\n",
      "y_test: (209, 2)\n",
      "y_val: (417, 2)\n"
     ]
    }
   ],
   "source": [
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "# testsize_train= 1-validation_size\n",
    "# testsize = 1 - validation_size\n",
    "seed=42\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.20)\n",
    "\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_test, \n",
    "                                               y_test, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.3)\n",
    "\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"X_val: \" + str(X_val.shape))\n",
    "print(\"y_train: \" + str(y_train.shape))\n",
    "print(\"y_test: \" + str(y_test.shape))\n",
    "print(\"y_val: \" + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming X_test, X_train, y_train, y_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = X_train.shape[0]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "num_test = X_test.shape[0]\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "num_val = X_val.shape[0]\n",
    "mask = list(range(num_val))\n",
    "X_val = X_val[mask]\n",
    "y_val = y_val[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the image data into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 30000) (209, 30000) (417, 30000)\n",
      "(2084, 2) (209, 2) (417, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data to zero mean, i.e centred around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_val -= mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the biais dimension of ones (i.e. biais trick) so that our SVM only has to worry about optimizing a single weight matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 30001) (209, 30001) (417, 30001)\n",
      "Data ready\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(\"Data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "X_train, X_test and X_val are centred around zero by subtracting the mean from the sets. A zero mean is a very common data pre-processing technique as it ensures that the gradients calculated remain controlled and to increase robustness to noise. It is important to note that the mean should be calculated only using the training data and not the validation or test sets. The biases are appended to weights as the last column of the weight matrix, so that now only one matrix W is to be optimized. This is also called the bias trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    y = [int(x) for x in y]\n",
    "    correct_class_scores = scores[np.arange(num_train), y].reshape(num_train, 1)\n",
    "    margin = np.maximum(0, scores - correct_class_scores + 1)\n",
    "    margin[np.arange(num_train), y] = 0  # do not consider correct class in loss\n",
    "    loss = margin.sum() / num_train\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "\n",
    "    margin[margin > 0] = 1\n",
    "    valid_margin = margin.sum(axis=1)\n",
    "    margin[np.arange(num_train), y] -= valid_margin\n",
    "    dW = (X.T).dot(margin) / num_train\n",
    "    # Regularization gradient\n",
    "    dW = dW + reg * 2 * W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "The svm_loss_vectorized function has the arguments W matrix that consists of the weights as well as the bias,input matrix X, target matrix y and reg the regularization strength. The scores metric is calculated according to W.X + b, which is reduced to W.X as the biases are included in matrix W with the bias trick. The loss is calculated from the average difference between the true target matrix y and the predicted scores. A further L2 regularization loss is added to encourage the weights to stay low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import object\n",
    "\n",
    "class LinearClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = float(np.max(y)) + 1.0 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, int(num_classes))\n",
    "        \n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            batch_indices = np.random.choice(num_train, batch_size, replace=False)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch=y[batch_indices]\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "\n",
    "            self.W -= learning_rate*grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = scores.argmax(axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:** ##\n",
    "The Linear Classifier class during training calculates the loss via the svm_loss_vectorized() function, which returns the loss as well as the gradient dw. The weights matrix is then updated with the equation W = W-a*dw. This process is repeated over several iterations called as epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-37ea2cc0c17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5e4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-ae83af230d0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# evaluate loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-fdb1e94f0484>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X_batch, y_batch, reg)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msvm_loss_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-568911df0c8c>\u001b[0m in \u001b[0;36msvm_loss_vectorized\u001b[0;34m(W, X, y, reg)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-568911df0c8c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorrect_class_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1500, verbose=True)\n",
    "\n",
    "y_train_pred = svmd.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred),))\n",
    "\n",
    "y_val_pred = svmd.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "An object of class LinearSVM is trained over X_train for 1500 epochs using some hyperparameters, learning rate and regularization strength. These hyperparameters will be tuned to achieve best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters to get the Best Fit\n",
    "\n",
    "#### Use the validation set to the hyperarameters (Regularizition strength and learning rate. You should experiment with different ranges for the learning rates and regularization strenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3]\n",
    "regularization_strengths = [2.5e4, 5e4, 2.5e3, 2.5e2, 5e2, 1e1]\n",
    "\n",
    "results = {}\n",
    "best_val = -1  # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None  # The LinearSVM object that achieved the highest validation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare blr, brg to store the best LinearSVM object's learning rate and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 5.000000e-07 reg 1.000000e+01 train accuracy: 0.983205 val accuracy: 0.491607\n",
      "lr 5.000000e-07 reg 2.500000e+02 train accuracy: 0.796065 val accuracy: 0.479616\n",
      "lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.965451 val accuracy: 0.496403\n",
      "lr 5.000000e-07 reg 2.500000e+03 train accuracy: 0.700096 val accuracy: 0.472422\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.551823 val accuracy: 0.448441\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.520633 val accuracy: 0.458034\n",
      "lr 5.000000e-06 reg 1.000000e+01 train accuracy: 0.995681 val accuracy: 0.494005\n",
      "lr 5.000000e-06 reg 2.500000e+02 train accuracy: 0.729367 val accuracy: 0.505995\n",
      "lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.630038 val accuracy: 0.472422\n",
      "lr 5.000000e-06 reg 2.500000e+03 train accuracy: 0.538868 val accuracy: 0.465228\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.510077 val accuracy: 0.534772\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.511036 val accuracy: 0.549161\n",
      "lr 5.000000e-05 reg 1.000000e+01 train accuracy: 0.850768 val accuracy: 0.450839\n",
      "lr 5.000000e-05 reg 2.500000e+02 train accuracy: 0.529271 val accuracy: 0.549161\n",
      "lr 5.000000e-05 reg 5.000000e+02 train accuracy: 0.527351 val accuracy: 0.458034\n",
      "lr 5.000000e-05 reg 2.500000e+03 train accuracy: 0.495202 val accuracy: 0.455635\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 1.000000e+01 train accuracy: 0.532630 val accuracy: 0.470024\n",
      "lr 5.000000e-04 reg 2.500000e+02 train accuracy: 0.511036 val accuracy: 0.537170\n",
      "lr 5.000000e-04 reg 5.000000e+02 train accuracy: 0.506718 val accuracy: 0.546763\n",
      "lr 5.000000e-04 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 1.000000e+01 train accuracy: 0.518714 val accuracy: 0.527578\n",
      "lr 5.000000e-03 reg 2.500000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "best validation accuracy achieved during cross-validation: 0.549161\n",
      "linear SVM on raw pixels final test set accuracy: 0.507177\n",
      "SVMD\n",
      "[0 1 1 0 0 0 1 0 1 0]\n",
      "actual\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "best_svm\n",
      "[1 0 0 1 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "blr = None\n",
    "brg = None\n",
    "\n",
    "grid_search = [(lr, rg) for lr in learning_rates for rg in regularization_strengths]\n",
    "for lr, rg in grid_search:\n",
    "    svmd = LinearSVM()\n",
    "    train_loss = svmd.train(X_train, y_train, learning_rate=lr, reg=rg,\n",
    "                            num_iters=2000, verbose=False)\n",
    "    # Predict values for training set\n",
    "    y_train_pred = svmd.predict(X_train)\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = np.mean(y_train_pred == y_train)\n",
    "    # Predict values for validation set\n",
    "    y_val_pred = svmd.predict(X_val)\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = np.mean(y_val_pred == y_val)\n",
    "    # Save results\n",
    "    results[(lr, rg)] = (train_accuracy, val_accuracy)\n",
    "    if best_val < val_accuracy:\n",
    "        blr = lr\n",
    "        brg = rg\n",
    "        best_val = val_accuracy\n",
    "        best_svm = svmd\n",
    "        \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "        lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)\n",
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=blr, reg=brg,\n",
    "                       num_iters=2000, verbose=False)\n",
    "y_svmd = svmd.predict(X_test)\n",
    "print(\"SVMD\")\n",
    "print(y_svmd[0:10])\n",
    "print(\"actual\")\n",
    "print(y_test[0:10])\n",
    "print(\"best_svm\")\n",
    "print(y_test_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Train set 0.997601\n",
      "Accuracy on Valid set 0.489209\n",
      "Accuracy on Test set 0.540670\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "classifier = svm.LinearSVC(penalty='l2', loss='squared_hinge', max_iter=2000)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Accuracy on Train set %f' % classifier.score(X_train, y_train))\n",
    "print('Accuracy on Valid set %f' % classifier.score(X_val, y_val))\n",
    "print('Accuracy on Test set %f' % classifier.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
