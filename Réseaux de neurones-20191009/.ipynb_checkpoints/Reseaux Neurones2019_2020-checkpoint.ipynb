{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Les réseaux de neurones **  \n",
    "\n",
    "** Les neurones **  \n",
    "\"Un neurone, ou une cellule nerveuse, est une cellule excitable constituant l'unité fonctionnelle de base du système nerveux. Les neurones assurent la transmission d'un signal bioélectrique appelé influx nerveux. Ils ont deux propriétés physiologiques : l'excitabilité, c'est-à-dire la capacité de répondre aux stimulations et de convertir celles-ci en impulsions nerveuses, et la conductivité, c'est-à-dire la capacité de transmettre les impulsions.\" (Wikipedia)\n",
    "\n",
    "La structure d'un neurone (source : https://fr.wikipedia.org/wiki/Fichier:Neurone_-_commenté.svg) :\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/Neurone.png' width=\"300\">\n",
    "\n",
    "Le fonctionnement est le suivant : tout d'abord, les dendrites reçoivent l’influx nerveux d’autres neurones. Le neurone évalue alors l’ensemble de la stimulation reçue. Si celle-ci est suffisante, il est excité : il transmet un signal (0/1) le long de l ’axone et l’excitation est propagée jusqu’aux autres neurones qui y sont connectés via les synapses.\n",
    "\n",
    "\"Un réseau de neurones artificiels, ou réseau neuronal artificiel, est un système dont la conception est à l'origine schématiquement inspirée du fonctionnement des neurones biologiques\" (Wikipedia)  \n",
    "\n",
    "\n",
    "La structure d'un neurone artificiel : \n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/NeuroneDansUnReseauDeNeurone.png' width=\"400\">\n",
    "\n",
    "Comme nous le constatons, un neurone artificiel est assez similaire à un neurone. Il comprend un ensemble d'entrées (synapses) auxquelles un ensemble de poids sont ajoutés (dans le notebook sur la descente de gradient, ces poids correspondent aux paramètres qu'il fallait trouver pour les fonctions linéaires - $\\theta$). Il possède également une entrée particulière appelée *biais*. Une fonction additive (combinaison linéaire)  calcule la somme pondérée des entrées : $\\sum_{i}^{} w_ix_i$. La sortie du noeud est déterminée en appliquant une fonction de transfert non-linéaire, $\\sigma(\\sum_{i}^{} w_ix_i+b)$.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorsque la régression logistique ne fonctionne plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le notebook sur la descente de gradient, nous avons terminé par la régression logistique et avons vu qu'il était possible d'afficher les limites de décision (une droite) pour classer les iris. Considérons, à présent, la figure suivante :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0) # pour avoir des figures de même taille\n",
    "np.random.seed(0)\n",
    "X, y = make_moons(n_samples=1000, noise=0.1)\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=cm_bright)#cmap=plt.cm.PiYG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from :\n",
    "#https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%201%20-%20Artificial%20Neural%20Networks.ipynb\n",
    "\n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons, à présent, la logistic regression de sickit learn pour afficher la frontière de decision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d34c21e9c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#Plot the decision boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X, y)\n",
    "#Plot the decision boundary\n",
    "plot_decision_boundary(lambda x: clf.predict(x),X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater les deux ensembles ne peuvent pas être séparés linéairement. Nous avons besoin de quelque chose de plus sophistiqué : les réseaux de neurones. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les réseaux de neurones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "Les réseaux de neurones se composent des éléments suivants :  \n",
    "\n",
    "* Une couche d'entrée qui reçoit l'ensemble des caractéristiques (features), i.e. les variables prédictives. \n",
    "\n",
    "* Un nombre arbitraire de couches cachées.  \n",
    "\n",
    "* Une couche de sortie, ŷ, qui contient la variable à prédire.  \n",
    "\n",
    "* Un ensemble de poids W qui vont être ajoutés aux valeurs des features et de biais b entre chaque couche  \n",
    "\n",
    "* Un choix de fonction d'activation pour chaque couche cachée, σ. \n",
    "\n",
    "*Remarque* La couche de sortie doit avoir autant de neurones qu'il y a de sorties au problème de classification :\n",
    "* *régression* : 1 seul neurone (C.f. notebook descente de gradient)\n",
    "* *classification binaire* : 1 seul neurone avec une fonction d'activation qui sépare les deux classes. \n",
    "* *classification multi-classe* : 1 neurone par classe et une fonction d'activation Softmax pour avoir la classe appropriée en fonction des probabilités de l'entrée appartenant à chaque classe.\n",
    "\n",
    "La figure suivante illustre un exemple de réseau avec 3 couches : \n",
    "* le layer 1 correspond au layer d'entrée (*input layer*), il reçoit l'ensemble des variables prédicives et est composé de 2 neurones. Le neurone avec +1 correspond au biais qui est ajouté.    \n",
    "* le layer 2 est appelé couche cachée (*hidden layer*), il possède 3 neurones et aussi un biais. \n",
    "* le layer 3 correspond à la couche de sortie (*output layer*), la sortie de ce layer correspond à la prédiction.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/unreseaudeep3.png' width=\"550\">\n",
    "\n",
    "\n",
    "La figure suivante illustre le même réseau avec les poids affectés.\n",
    "\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/unreseaudeeppoids6.png' width=\"550\">\n",
    "\n",
    "\n",
    "**Notations** : $X$ correspond aux variables prédictives. Le poids est identifié de la manière suivante : $w_{ij}^{[l]}$ où $l$ correspond au niveau du layer cible, $i$ correspond au numéro du nœud de la connection dans la couche $l-1$ et $j$ correspond au numéro du nœud de la connection dans la couche $l$. Par exemple, le poids entre le nœud 1 dans le layer 1 et le nœud 2 dans le layer 2 est noté : $w_{12}^{[2]}$. Un biais est connecté à chaque nœud de la couche suivante. La notation est similaire : $b_i^{[l]}$ où $i$ est le numéro du nœud de la couche supérieure. La sortie d'un nœud est notée = $a_i^{[l]}$ où $i$ correspond au numéro du nœud dans la couche $l$. **ŷ** correspond à la variable prédite. \n",
    " \n",
    " \n",
    "\n",
    " \n",
    "**Choix de la fonction d'activation** \n",
    "\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/Activations.png' width=\"600\">\n",
    "\n",
    "\n",
    "Il existe de très nombreuses fonctions d'activation qui peuvent être utilisées :\n",
    "* Binary Step  \n",
    "*  Sigmoid  \n",
    "*  Tanh  \n",
    "*  ReLU  \n",
    "* Leaky ReLU  \n",
    "* Softmax  \n",
    "* ...  \n",
    "\n",
    "\n",
    "Elles n'ont pas les mêmes propriétés.   \n",
    "\n",
    "Nous verrons, par la suite, que les réseaux de neurones utilisent la descente de gradient, le comportement de la dérivée des fonctions est donc important. Par exemple, nous avons vu que la sigmoid va transformer de grandes valeurs d'entrée dans des valeurs comprises entre 0 et 1. Cela veut dire qu'une modification importante de l'entrée entraînera une modification mineure de la sortie (C.f. notebook descente de gradient). Par conséquent la dérivée devient plus petite comme l'illustre l'image ci-dessous :   \n",
    "\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/sigmoid_derivative.png' width=\"400\">  \n",
    "\n",
    "En fait, pour corriger les erreurs, les dérivées du réseau vont être propagées layer par layer de l'output layer à l'input layer. Le problème est que celles-ci sont multipliées entre chaque layer afin de connaître les valeurs de dérivées utiles pour l'input layer : le gradient décroît de façon exponentielle à mesure que nous nous propagons jusqu'aux couches initiales.\n",
    "\n",
    "\n",
    "Pour choisir les fonctions d'activation, il faut considérer les propriétés principales suivantes :  \n",
    "\n",
    "* La disparition du gradient (*vanishing gradient*) : le problème intervient généralement dans des réseaux avec de très nombreux layer. Comme les descentes de gradient sont propagées dans tout le réseau, de trop petites valeurs de gradient (le gradient de la fonction de perte approche 0) indiquent que les poids des premiers layers ne seront pas mis à jour efficacement à chaque étape. Ceci entraîne donc une imprécision globale du réseau. Cela peut arriver si le réseau est composé de nombreuses couches avec une sigmoid.\n",
    "* Disparition de neurones (*dead neuron*) : un neurone mort est un neurone qui, lors de l'apprentissage, ne s'active plus. Cela est lié au fait que les dérivées sont très petites ou nulles. Le neurone ne peut donc pas mettre à jour les poids. Les erreurs ne se propageant plus, ce neurone peut affecter les autres neurones du réseau. C'est, par exemple, le cas avec ReLu qui renvoie 0 quand l'entrée est inférieure ou égale à 0. Si chaque exemple donne une valeur négative, le neurone ne s'active pas et après la descente de gradient le neurone devient 0 donc ne sera plus utilisé. Le Leaky Relu permet de résoudre ce problème.  \n",
    "* Explosion du gradient (*Explosing gradient*) : le problème se pose lorsque des gradients d'erreur important s'accumulent et entraînent des mises à jour importantes des poids. Cela amène un réseau instable : les valeurs de mises à jour des poids peuvent être trop grandes et être remplacées par des NaN donc non utilisables (s'il n'y a pas d'erreurs d'exécution bien sûr !). Le problème est lié au type de descente de gradient utilisé (Batch vs mini-batch), au fait qu'il y a peut être trop de couches dans le réseau et bien sûr à certaines fonctions d'activation qui favorisent ce problème.  \n",
    "* Saturation de neurones (*Saturated neurons*) : le problème est lié au fait que les valeurs grandes (resp. petites) atteignent un plafond et qu'elles ne changent pas lors de la propagation dans le réseau. Ce problème est principalement lié aux fonctions sigmoid et tanh. En effet, sigmoid, pour toutes les valeurs supérieures à 1 va arriver sur un plateau et retournera toujours 1. Pour cela, ces deux fonctions d'activations sont assez déconseillées en deep learning (préférer LeRu ou Leaky Relu).  \n",
    "\n",
    "\n",
    "\n",
    "Pour avoir une idée du comportement des différentes fonctions d'activation et de leurs conséquences : \n",
    "’’\n",
    "\n",
    "\n",
    "Une fois que tout est fixe, le réseau de neurones s'exécute alors en deux étapes : \n",
    "* Forward Propagation\n",
    "* Backward Propagation  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette étape est de déterminer la valeur de sortie du réseau : **ŷ**. \n",
    "\n",
    "\n",
    "Comme nous avons vu dans le notebook descente de gradient, pour chaque neurone de la couche, nous effectuons une application affine en considérant les valeurs issues de la couche précédente (i.e. $a$ représente le résultat de la fonction d'activation de la couche précédente) :  \n",
    "\n",
    "\n",
    "$$\\mathbf{z}_i^{[l]}=\\mathbf{w}^T_i \\bullet \\mathbf{a}^{[l-1]}+b_i\\ \\ \\ \\ \\ \\  \\mathbf{a}_i^{[l]}=\\sigma^{[l]}(\\mathbf{z}_i^{[l]})$$\n",
    "\n",
    "Que nous pouvons donc généraliser en utilisant les matrices :  \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\mathbf{Z}^{[l]} = \\mathbf{W}^{[l]} \\bullet \\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}\\\\  \n",
    "\\mathbf{A}^{[l]}=\\sigma^{[l]}(\\mathbf{Z}^{[l]})\\\\\n",
    "\\end{array}\n",
    "$$  \n",
    "\n",
    "Si nous reprenons l'exemple de réseau précédent avec ReLu pour le hidden layer et sigmoid pour le layer de sortie nous avons donc :   \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\mathbf{A}^{[0]}=X\\\\\n",
    "\\end{array}\n",
    "$$  \n",
    " \n",
    "où $\\mathbf{A}^{[0]}=X$ la matrice contenant les exemples d'apprentissage.  \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]} \\bullet \\mathbf{A}^{[0]} + \\mathbf{b}^{[1]}\\\\\n",
    "\\mathbf{A}^{[1]}=ReLu^{[1]}(\\mathbf{Z}^{[1]})\\\\\n",
    "\\mathbf{Z}^{[2]} = \\mathbf{W}^{[2]} \\bullet \\mathbf{A}^{[1]} + \\mathbf{b}^{[2]}\\\\\n",
    "\\mathbf{A}^{[2]}=Sigmoid^{[2]}(\\mathbf{Z}^{[2]})\\\\  \n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "Finalement : $ŷ=\\mathbf{A}^{[2]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous illustre un exemple simple de la phase forward propagation pour notre exemple comportant deux variables prédictives. Les fonctions d'activations sont respectivement Relu pour le hidden layer et  sigmoid pour le dernier layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#fonctions d'activation\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def valueoutput(y_hat):\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_hat[i]>0.5:\n",
    "            y_hat[i]=1\n",
    "        else: \n",
    "            y_hat[i]=0\n",
    "    return y_hat        \n",
    "\n",
    "# les donnees d'entree sous la forme d'une matrice (array en python)\n",
    "X = np.array(([0, 1], \n",
    "               [0, 0],\n",
    "               [1, 0],\n",
    "               [1, 1],\n",
    "               [1, 1])) \n",
    "# les donnees de sortie sous la forme d'un vecteur\n",
    "y = np.array(([1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1]), dtype=float) \n",
    "\n",
    "\n",
    "# initialisation des poids de manière aléatoire ainsi que des biais\n",
    "inputSize = 2\n",
    "hiddenSize = 3\n",
    "outputSize = 1\n",
    "W1=np.random.rand(2, 3)\n",
    "W2=np.random.rand(3, 1)\n",
    "b1 = np.random.rand(3)\n",
    "b2 = np.random.rand(1)\n",
    "print (\"Les données d'entrées : \\n\",X)\n",
    "print ('Les valeurs de poids et de biais initialisees aléatoirement : \\n')\n",
    "print ('\\t(layer input vers layer 1) : W1 \\n',W1,'\\n')\n",
    "print ('\\t(layer input vers layer 1) : b1\\n',b1,'\\n')\n",
    "print (\"\\t(layer 1 vers layer 2) : W2\\n\",W2,'\\n')\n",
    "print (\"\\t(layer 1 vers layer 2) : b2\\n\",b2,'\\n')\n",
    "print (\"Etape 1 : \")\n",
    "print (\"\\n A0=X\\n\")\n",
    "\n",
    "A0=X\n",
    "Z1 = np.dot(A0,W1)+b1\n",
    "print (\"\\n Z1 = W1.A0 + b1  \\n\",Z1,'\\n')\n",
    "A1 = relu(Z1) \n",
    "print ('\\nA1 = relu(Z1)\\n',A1,'\\n')\n",
    "Z2 = np.dot(A1,W2)+b2\n",
    "print (\"\\n Z2 = W2.A1 + b2  \\n\",Z2,'\\n')\n",
    "A2 = sigmoid(Z2) \n",
    "print ('\\nA2 = sigmoid(Z2)\\n',A2,'\\n')\n",
    "y_hat = sigmoid(Z2) \n",
    "print ('yhat\\n',y_hat)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Les données d'entrées : \\n\",X)\n",
    "print (\"Les sorties predites : \\n\", str(valueoutput(y_hat)))\n",
    "\n",
    "print (\"Les sorties reelles attendues : \\n\", str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater il y a des erreurs dans les sorties prédites. C'est là qu'intervient la seconde phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de la Backward Propagation est tout d'abord d'évaluer la différence entre la valeur prédite et la valeur réelle.   \n",
    "\n",
    "*Etape 1 : (calcul du coût)*\n",
    "\n",
    "Nous avons vu dans le notebook de la descente de gradient que la différence entre la valeur obtenue dans l'étape précédente et la valeur réelle correspond au coût. Plus la différence est élevée, plus le coût sera élevé. Pour minimiser ce coût, il faut trouver les valeurs de poids et de biais pour lesquelles la fonction de coût renvoie la plus petite valeur possible. Plus le coût est faible, plus les prévisions sont exactes. Nous retrouvons donc le problème rencontré pour la descente de gradient. \n",
    "\n",
    "Précédemment nous avons vu que la cross entropy, comme fonction de coût, était bien adaptée à notre problème de classification binaire, donc :  \n",
    "\n",
    "$$\\mathbf{C(ŷ,y)=-ylog(ŷ)-(1-y)log(1-ŷ)}$$ \n",
    "\n",
    "L'objectif, à présent, est de propager cette erreur dans tout le réseau pour mettre à jour les différents poids.\n",
    "\n",
    "\n",
    "*Etape 2 : (backpropagation)  *  \n",
    "**Comprendre ce qui est derrière**  \n",
    "\n",
    "Nous avons vu précédemment, lors de la phase de forward, que l'exécution était de la forme :  \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\mathbf{A}^{[0]}=X\\\\ \n",
    "\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]} \\bullet \\mathbf{A}^{[0]} + \\mathbf{b}^{[1]}\\\\\n",
    "\\mathbf{A}^{[1]}=\\sigma^{[1]}(\\mathbf{Z}^{[1]})\\\\\n",
    "\\mathbf{Z}^{[2]} = \\mathbf{W}^{[2]} \\bullet \\mathbf{A}^{[1]} + \\mathbf{b}^{[2]}\\\\\n",
    "\\mathbf{A}^{[2]}=\\sigma^{[2]}(\\mathbf{Z}^{[2]})\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{Z}^{[L]} = \\mathbf{W}^{[L]}\\bullet \\mathbf{A}^{[L-1]} + \\mathbf{b}^{[L]}\\\\\n",
    "\\mathbf{A}^{[L]}=\\sigma^{[L]}(\\mathbf{Z}^{[L]})=ŷ\\\\\n",
    "\\end{array}$$  \n",
    "\n",
    "où $L$ est le output layer.  \n",
    "\n",
    "La figure suivante illustre les étapes jusqu'à la fonction de coût pour notre réseau exemple :  \n",
    "\n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/ForwardVertical2.png' width=\"400\">\n",
    "\n",
    "\n",
    "L'objectif de la backward propagation est de reporter, dans le réseau, l'ensemble des modifications à apporter aux poids entre les couches. Pour cela il faut repartir en sens inverse pour calculer les dérivées partielles du coût. Elle repose sur la règle de dérivation en chaîne (*chain rule*) qui est une formule qui explicite la dérivée d'une fonction composée pour deux fonctions dérivables :  \n",
    "\n",
    "$$\\mathbf{\\frac{dy}{dx}=\\frac{dy}{du}\\frac{du}{dx}}$$  \n",
    "\n",
    "Lorsque l'on regarde la fin du réseau, nous constatons que $\\mathbf{C}$ est une fonction qui dépend de $\\mathbf{A^{[2]}}$, que $\\mathbf{A^{[2]}}$ dépend, elle même, d'une fonction $\\mathbf{Z^{[2]}}$ et que finalement $\\mathbf{Z^{[2]}}$ dépend de $\\mathbf{W^{[2]}}$ et de  $\\mathbf{b^{[2]}}$. \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial W^{[2]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[2]}}}\\bullet \\mathbf{\\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}}$$  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial b^{[2]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[2]}}}\\bullet \\mathbf{\\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[2]}}{\\partial b^{[2]}}}$$  \n",
    "\n",
    "\n",
    "\n",
    "De la même manière, pour avoir la dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{W^{[1]}}$ et   $\\mathbf{b^{[1]}}$, nous voyons, sur la figure, que $\\mathbf{Z^{[2]}}$ est une fonction qui dépend de $\\mathbf{A^{[1]}}$, qui, elle-même, dépend de $\\mathbf{Z^{[1]}}$ et que finalement $\\mathbf{Z^{[1]}}$ dépend de $\\mathbf{W^{[1]}}$ et $\\mathbf{b^{[1]}}$.  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial W^{[1]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[2]}}}\\bullet \\mathbf{\\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}}\\bullet\\mathbf{\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}}$$\n",
    "\n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial b^{[1]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[2]}}}\\bullet \\mathbf{\\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}}\\bullet\\mathbf{\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}}\\bullet\\mathbf{\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}}}$$  \n",
    "\n",
    "Les différentes étapes sont résumées sur la figure suivante :  \n",
    "\n",
    "<img src='https://www.lirmm.fr/~poncelet/Images/BackwardVertical2.png' width=\"400\">  \n",
    "\n",
    "Pour résumer, les équations pour calculer la dérivée partielle de la fonction de coût en fonction des poids et des biais d'une couche  $\\mathbf{l}$ sont : \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial W^{[l]}}}=\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}}\\bullet \\mathbf{\\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}}$$\n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial b^{[l]}}}=\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}}\\bullet \\mathbf{\\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}}$$  \n",
    "\n",
    "Donc, pour obtenir les dérivées partielles de $\\mathbf{C}$ par rapport à $\\mathbf{W^{[l]}}$ et \n",
    "$\\mathbf{b^{[l]}}$, nous devons calculer :  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}},\\frac{\\partial C}{\\partial Z^{[L]}},\\frac{\\partial C}{\\partial Z^{[l]}}\\frac{\\partial Z^{[l]}}{\\partial W^{[l]}},\\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}}$$  \n",
    "\n",
    "\n",
    "Par la suite, et par simplification, nous considérons que les deux fonctions d'activation dans notre réseau sont Relu (pour $A^{[1]}$) et sigmoid (pour $A^{[2]}$). Le principe est le même quelques soient les fonctions, il suffit juste de connaître la dérivée des fonctions d'activation.    \n",
    "\n",
    "\n",
    "\n",
    "**Pour la dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{A^{[L]}}$ :**  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}}=\n",
    "\\mathbf{\\frac{\\partial (-ylog(A^{[L]})-(1-y)log(1-A^{[L]}))}{\\partial A^{[L]}}}$$\n",
    "\n",
    "Nous savons que la dérivée d'une fonction log(x) est :\n",
    "$$\\mathbf{\\frac{\\partial log(x)}{\\partial dx}}=\\mathbf{\\frac{1}{x}}$$  \n",
    "\n",
    "Pour la partie gauche $\\mathbf{-ylog(A^{[L]})}$ nous avons donc comme dérivée :  \n",
    "$$\\mathbf{\\frac{-y}{A^{[L]}}}$$\n",
    "\n",
    "\n",
    "Pour la partie droite $\\mathbf{-(1-y)log(1-A^{[L]})}$, il faut juste appliquer la formule de la dérivée d'une fonction  :\n",
    "\n",
    "$$\\mathbf{\\frac{\\partial log(g(x))}{\\partial dx}}=\\mathbf{\\frac{1}{g(x)}g'(x))}$$  \n",
    "comme la dérivée de $\\mathbf{1-A^{[L]}}$ est $\\mathbf{-1}$ nous avons au final : \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}}=\\mathbf{\\frac{-y}{A^{[L]}}}-(-)\\mathbf{\\frac{(1-y)}{(1-A^{[L]})}}$$  \n",
    "\n",
    "\n",
    "$$=\\mathbf{\\left(\\frac{-y}{A^{[L]}}+\\frac{(1-y)}{(1-A^{[L]})}\\right)}$$  \n",
    "\n",
    "Donc :   \n",
    "\n",
    "\n",
    "$$\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}} = \\mathbf{\\left(\\frac{-y}{A^{[L]}}+\\frac{(1-y)}{(1-A^{[L]}})\\right)}\\\\\n",
    "\\hline\n",
    "\\end{array}$$  \n",
    "\n",
    "**Considérons, à présent la dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{Z^{[L]}}$ : ** \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}}$$  \n",
    "\n",
    "En utilisant la chaîne de dérivation :  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}}\\bullet \\mathbf{\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}}=\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}*\\sigma'^{[L]}(Z^{[L]})}$$\n",
    "\n",
    "$\\mathbf{\\sigma'^{[L]}(Z^{[L]})}$ correspond simplement à la dérivée de la sigmoid. Nous avons vu dans le notebook sur la descente de gradient, que cette dérivée est :   \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}}=\\mathbf{sigmoid(Z^{[L]})(1-sigmoid(Z^{[L]}))}=\\mathbf{A^{[L]}(1-A^{[L]})}$$\n",
    "\n",
    "Donc :  \n",
    "\n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}}\\bullet \\mathbf{\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}}=\\mathbf{\\left(\\frac{-y}{A^{[L]}}+\\frac{(1-y)}{(1-A^{[L]})}\\right)}\\mathbf{A^{[L]}(1-A^{[L]})}$$  \n",
    "\n",
    "Nous pouvons multiplier par $\\mathbf{(1-A^{[L]})}$ et $\\mathbf{(A^{[L]})}$ pour simplifier : \n",
    "$$=\\mathbf{\\left(\\frac{-y(1-A^{[L]})}{A^{[L]}(1-A^{[L]})}+\\frac{A^{[L]}(1-y)}{A^{[L]}(1-A^{[L]})}\\right)}\\mathbf{A^{[L]}(1-A^{[L]})}$$  \n",
    "\n",
    "$$=\\mathbf{\\left(\\frac{-y(1-A^{[L]})+A^{[L]}(1-y)}{A^{[L]}(1-A^{[L]})}\\right)}\\mathbf{A^{[L]}(1-A^{[L]})}$$ \n",
    "\n",
    "\n",
    "en supprimant $\\mathbf{A^{[L]}(1-A^{[L]})}$ nous avons :  \n",
    "$$=\\mathbf{\\left(-y(1-A^{[L]})+A^{[L]}(1-y)\\right)}$$ \n",
    "\n",
    "$$=\\mathbf{-y+yA^{[L]}+A^{[L]}-A^{[L]}y}$$  \n",
    "\n",
    "$$=\\mathbf{-y+A^{[L]}}$$  \n",
    "\n",
    "Donc : \n",
    "\n",
    "$$\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}}=\\mathbf{A^{[L]}-y}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "**Dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{Z^{[l]}}$ :**  \n",
    "Nous souhaitons, à présent, obtenir la dérivée partielle de $\\mathbf{C}$ par rapport à un niveau $\\mathbf{l}$, i.e. $\\mathbf{Z^{[l]}}$. Le principe étant que si l'on connaît $\\mathbf{Z^{[L]}}$, il est possible de déduire $\\mathbf{Z^{[L-1]}}$, $\\mathbf{Z^{[L-2]}}$, ...  \n",
    "\n",
    "Nous savons, en appliquant la chaîne de dérivation, qu'il est possible de calculer la dérivée de $\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}}$ par :  \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}=\\frac{\\partial C}{\\partial Z^{[l+1]}}\\bullet\\frac{\\partial Z^{[l+1]}}{\\partial A^{[l]}}\\bullet\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} }$$\n",
    "\n",
    "Comme :\n",
    "$$\\mathbf{Z^{[l+1]}=W^{[l+1]}\\bullet A{[l]}+b{[l+1]}}$$ \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial Z^{[l+1]}}{\\partial A^{[l]}}= \\frac{\\partial (W^{[l+1]}\\bullet A{[l]}+b{[l+1]})}{\\partial A^{[l]}}}$$  \n",
    "\n",
    "$$\\mathbf{=W^{[l+1]}}$$\n",
    "\n",
    "Nous avons également : \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}}= \\sigma'^{[l]}(Z^{[l]})}$$\n",
    "\n",
    "Donc :  \n",
    "\n",
    "$$\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}} =(W^{[l+1]^T} \\bullet \\frac{\\partial C}{\\partial Z^{[l+1]}})*\\sigma'^{[l]}(Z^{[l]})}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "*Dérivée partielle de $\\mathbf{Z^{[l]}}$ par rapport à $\\mathbf{W^{[l]}}$ :*  \n",
    "\n",
    "Dans un premier temps nous calculons la dérivée partielle de $\\mathbf{Z^{[l]}}$ par rapport à $\\mathbf{W^{[l]}}$ pour, par la suite déterminer la dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{W^{[l]}}$.  \n",
    "\n",
    "Comme :\n",
    "$$\\mathbf{Z^{[l]}=W^{[l]}\\bullet A{[l-1]}+b{[l]}}$$ \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial Z^{[l]}}{\\partial W^{[l]}} =\\frac{\\partial (W^{[l]}\\bullet A{[l-1]}+b{[l]})}{\\partial W^{[l]}}}$$\n",
    "\n",
    "$$\\mathbf{= A^{[l-1]}}$$\n",
    "\n",
    "**Dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{W^{[l]}}$ :**  \n",
    "\n",
    "A partir du résultat précédent, nous avons :\n",
    "\n",
    "$$\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial W^{[l]}} =\\frac{\\partial C}{\\partial Z^{[l]}}\\bullet A^{[l-1]^T}}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "*Dérivée partielle de $\\mathbf{Z^{[l]}}$ par rapport à $\\mathbf{b^{[l]}}$ :*  \n",
    "\n",
    "Comme précédemment, nous calculons la dérivée partielle de $\\mathbf{Z^{[l]}}$ par rapport à $\\mathbf{b^{[l]}}$ pour, par la suite déterminer la dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{b^{[l]}}$.\n",
    "\n",
    "Comme :\n",
    "$$\\mathbf{Z^{[l]}=W^{[l]}\\bullet A{[l-1]}+b{[l]}}$$ \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial Z^{[l]}}{\\partial b^{[l]}} =\\frac{\\partial (W^{[l]}\\bullet A{[l-1]}+b{[l]})}{\\partial b^{[l]}}}$$\n",
    "\n",
    "$$\\mathbf{= 1}$$  \n",
    "\n",
    "**Dérivée partielle de $\\mathbf{C}$ par rapport à $\\mathbf{b^{[l]}}$ :**  \n",
    "\n",
    "A partir du résultat précédent, nous avons :  \n",
    "\n",
    "\n",
    "$$\\begin{array}{|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial b^{[l]}} =\\frac{\\partial C}{\\partial Z^{[l]}}}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "** Pour résumer **\n",
    "\n",
    "A présent, pour le dernier layer $\\mathbf{L}$, nous sommes capable de calculer la dérivée partielle du coût par rapport à $\\mathbf{A^{[L]}}$, $\\mathbf{Z^{[L]}}$, $\\mathbf{W^{[L]}}$ et $\\mathbf{b^{[L]}}$ :\n",
    "\n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial A^{[L]}}} & \\mathbf{\\left(\\frac{-y}{A^{[L]}}+\\frac{(1-y)}{(1-A^{[L]})}\\right)}\\\\\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}} & \\mathbf{(A^{[L]}-y)}\\\\\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial W^{[L]}}} & \\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}\\bullet (A^{[L-1]^T})}\\\\\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial b^{[L]}}} & \\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "Pour n'importe quel layer $\\mathbf{l}$, nous avons : \n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}} & \\mathbf{(W^{[l+1]^T} \\bullet \\frac{\\partial C}{\\partial Z^{[l+1]}})*\\sigma'^{[l]}(Z^{[l]})}\\\\\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial W^{[l]}}} & \\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}\\bullet A^{[l-1]^T}}\\\\\n",
    "\\hline\n",
    "\\mathbf{\\frac{\\partial C}{\\partial b^{[l]}}} & \\mathbf{\\frac{\\partial C}{\\partial Z^{[l]}}}\\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "\n",
    "**La descente de gradient**  \n",
    "\n",
    "Attention, parfois, la backward propagation est considérée comme la descente de gradient. Ce n'est pas le cas. Elle a pour seul objectif de calculer les gradients pour les opérations à chaque niveau. La descente de gradient intervient après, elle permet de pouvoir mettre automatiquement les poids des différents layer en appliquant justement les gradients obtenus dans l'étape précédente. \n",
    "\n",
    "\n",
    "La descente de gradient se fait comme dans le notebook : il faut boucler jusqu'au premier layer pour appliquer la formule du gradient à l'aide des dérivées calculées précédemment :  \n",
    "\n",
    "\n",
    "**For $\\mathbf{l}$ in enumate (dernier_layer,1) { ** \n",
    "$$\\mathbf{W^{[l]}=W^{[l]}-\\eta \\frac{\\partial C}{\\partial W^{[l]}}}$$\n",
    "$$\\mathbf{b^{[l]}=b^{[l]}-\\eta \\frac{\\partial C}{\\partial b^{[l]}}}$$ \n",
    "**}**  \n",
    "\n",
    "Remarque : comme nous l'avons vu lors des dérivations, il est nécessaire de sauvegarder $\\mathbf{\\frac{\\partial C}{\\partial W^{[l]}}}$ et $\\mathbf{\\frac{\\partial C}{\\partial b^{[l]}}}$ pour pouvoir les réutiliser lors de la descente de gradient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas de la classification multi-classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jusqu'à présent nous avons vu comment faire de la classification binaire, i.e. la fonction d'activation est une sigmoid. Pour faire de la classification multi-classe, il faut utiliser la fonction d'activation softmax. Elle attribue des probabilités à chaque classe d'un problème à plusieurs classes et la somme de ces probabilités doit être égale à 1. Formellement softmax, prend en entrée un vecteur de C-dimensions (le nombre de classes possibles) $\\mathbf{z}$ et retourne un autre vecteur de C-dimensions $\\mathbf{a}$ de valeurs réelles comprises entre 0 et 1. \n",
    "\n",
    "Pour $i=1 \\cdots C$ :\n",
    "$$\\mathbf{a_i=\\frac{e^{z_i}}{\\sum_{k=1}^{C}e^{z_k}}}$$\n",
    "$$ avec\\ \\sum_{i=1}^{C}=1$$\n",
    "\n",
    "où $C$ correspond au nombre de classes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    expz = np.exp(z)\n",
    "    return expz / expz.sum(axis=0, keepdims=True)\n",
    "\n",
    "nums = np.array([4, 5, 6])\n",
    "print(softmax(nums))\n",
    "print (\"la somme des probabilités donne 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, cette fonction n'est pas très stable : elle génère souvent des nan pour des grands nombres par exemple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.array([4000, 5000, 6000])\n",
    "print(softmax(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi il est fréquent de multiplier le numérateur par une constante : généralement $-max(z)$ :  \n",
    "\n",
    "$$\\mathbf{a_i=\\frac{e^{z_i-max(z)}}{\\sum_{k=1}^{C}e^{z_k-max(z)}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    expz = np.exp(z - np.max(z))\n",
    "    return expz / expz.sum(axis=0, keepdims=True)\n",
    "\n",
    "nums = np.array([4, 5, 6])\n",
    "print(softmax(nums))\n",
    "print (\"la somme des probabilités donne 1\")\n",
    "nums = np.array([4000, 5000, 6000])\n",
    "print(softmax(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dérivée de la fonction softmax**\n",
    "\n",
    "Elle est basée sur le fait de considérer la ré-écriture suivante :  \n",
    "\n",
    "$$\\mathbf{g(x)=e^{z_i}}$$\n",
    "et\n",
    "$$\\mathbf{h(x)=\\sum_{k=1}^{C}e^{z_k}}$$\n",
    "\n",
    "Nous savons que la dérivée d'une fonction $\\mathbf{f(x)=\\frac{g(x)}{h(x)}}$ est :  \n",
    "\n",
    "$$\\mathbf{f'(x)=\\frac{g'(x)h(x)-h'(x)g(x)}{h(x)^2}}$$\n",
    "\n",
    "Par simplification, nous notons : \n",
    "$$\\mathbf{\\sum_C=\\sum_{k=1}^{C}e^{z_k}}$$\n",
    "\n",
    "Pour $i=1 \\cdots C$, nous avons : \n",
    "\n",
    "$$\\mathbf{a_i=\\frac{e^{z_i}}{\\sum_{C}}}$$\n",
    "\n",
    "La dérivée $\\mathbf{\\frac{\\partial a_i}{\\partial z_j}}$ de la sortie de softmax $\\mathbf{a}$ par rapport à $\\mathbf{z}$ :\n",
    "\n",
    "* Si $\\mathbf{i=j}$,\n",
    "$$\\mathbf{\\frac{\\partial a_i}{\\partial z_i}=\\frac{\\partial (\\frac{e^{z_i}}{\\sum_{C}})}{\\partial z_i}\n",
    "=\\frac{e^{z_i}\\sum_{C}-e^{z_i}e^{z_i}}{\\sum_{C}^2}=\\frac{e^{z_i}}{\\sum_{C}}\\frac{\\sum_{C}-e^{z_i}}{\\sum_{C}}=\n",
    "\\frac{e^{z_i}}{\\sum_{C}}(1-\\frac{e^{z_i}}{\\sum_{C}})=a_i(1-a_i)}$$\n",
    "Remarque : il s'agit du même résultat que pour la dérivée de la sigmoid.  \n",
    "\n",
    "* Si $\\mathbf{i \\ne j}$,\n",
    "$$\\mathbf{\\frac{\\partial a_i}{\\partial z_j}=\\frac{\\partial (\\frac{e^{z_i}}{\\sum_{C}})}{\\partial z_j}\n",
    "=\\frac{0-e^{z_i}e^{z_j}}{\\sum_{C}^2}=\\frac{e^{z_i}}{\\sum_{C}}\\frac{e^{z_j}}{\\sum_{C}}=-a_ia_j}$$  \n",
    "\n",
    "** Dérivée par rapport à la fonction de coût **  \n",
    "\n",
    "En appliquant le même principe que précédemment, on trouve : \n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial Z^{[L]}}}=\\mathbf{A^{[L]}-y}$$\n",
    "\n",
    "Donc toutes les dérivées précédentes pour $\\mathbf{W}$ et $\\mathbf{b}$ sont également similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est par contre nécessaire de redéfinir la fonction de prédiction. Généralement lorsqu'il y a plusieurs classes, il convient de transformer le $\\mathbf{y}$ initial en utilisant la fonction *OneHotEncoder*. Parfois, au préalable, il est indispensable de transformer les labels s'il s'agit d'attributs catégoriels en nombre via *LabelEncoder*. (Cf. notebook ingénierie des données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Transformation du label\n",
    "labelencoder = LabelEncoder()\n",
    "exemple=np.array([\"positif\",\"negatif\",\"positif\",\"negatif\"])\n",
    "print (\"Exemple \",exemple)\n",
    "label_encoded=labelencoder.fit_transform(exemple)\n",
    "print (\"labels encodés :\",label_encoded)\n",
    "\n",
    "\n",
    "# Encodage via OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "#il est indispensable de faire un reshape\n",
    "\n",
    "integer_encoded = label_encoded.reshape(len(label_encoded), 1)\n",
    "final = onehot_encoder.fit_transform(integer_encoded)\n",
    "print (\"Encodage final :\\n\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la prédiction, l'objectif est de retourner la classe qui a la plus forte probabilité à la sortie de softmax. Pour cela nous pouvons utiliser la fonction *argmax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple = np.array ([[0,2,25,4],\n",
    "                    [1,11,8,10],\n",
    "                    [200,7,5,10]])\n",
    "print (exemple)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Position des différents éléments dans la matrice\n",
    "                     #0     #1  #2  #3\n",
    "exemple = np.array ([[0,    2,  25,  4],  #0\n",
    "                     [1,   11,   8, 10],  #1\n",
    "                     [200,  7,   5, 10]]  #2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nPosition du plus grand élément : \",np.argmax(exemple))\n",
    "print (\"200 est le 8 ième élement (on compte à partir de 0)\\n\")  \n",
    "\n",
    "print (\"Axis = 0 : la fonction cherche la valeur maximale sur les colonne de la matrice\")\n",
    "print(\"\\nIndice de l'élément max en considérant les colonnes : \", np.argmax(exemple, axis = 0)) \n",
    "print (\"colonne 0 : 200 est le plus grand de la colonne (retourne ligne 2)\")\n",
    "print (\"colonne 1 : 11 est le plus grand de la colonne (retourne ligne 1)\")\n",
    "print (\"colonne 2 : 25 est le plus grand de la colonne (retourne ligne 0)\")\n",
    "print (\"colonne 3 : 10 est le plus grand de la colonne (retourne ligne 1)\\n\")\n",
    "\n",
    "print (\"Axis = 1 : la fonction cherche la valeur maximale sur les lignes de la matrice\")\n",
    "print(\"\\nIndices of Max element : \", np.argmax(exemple, axis = 1))\n",
    "print (\"ligne 0 : 25 est le plus grand de la colonne (retourne colonne 2)\")\n",
    "print (\"ligne 1 : 11 est le plus grand de la colonne (retourne colonne 1)\")\n",
    "print (\"ligne 2 : 200 est le plus grand de la colonne (retourne colonne 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implémentation sous la forme de fonctions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des librairies utiles. Dans la mesure du possible il est préférable de les mettre au tout début. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importations utiles\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0) # pour avoir des figures de même taille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps il est nécessaire de construire le réseau en indiquant le nombre de layers, de neurones par layer et les fonctions d'activation. Nous stockons ici cette information sous la forme d'un dictionnaire python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reseau de l'exemple du notebook\n",
    "# il contient trois layer : input - hidden - output\n",
    "#    par défaut on ne spécifie pas la couche d'entrée (couche 0) qui contient uniquement les \n",
    "#    hidden layer : elle reçoit en entrée les variables prédictives X avec des poids et un biais\n",
    "#              (input_dim=2) elle contient 5 neurones, la fonction d'activation est relu\n",
    "#    output layer : les dimensions d'entrée = 5 (il s'agit de la sortie de l'hidden layer)\n",
    "#                   elle donne le résultat. Ici la fonction d'activation est sigmoid car \n",
    "#                   classification binaire\n",
    "#               \n",
    "\n",
    "layers = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet de créer le réseau à partir du dictionnaire passé en paramètre. Elle initialise avec un nombre aléatoire les poids et le biais. Les différents paramètres du réseau (poids) et biais sont stockés sous la forme d'un dictionnaire python avec une clé qui indentifie le layer auquel il appartient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(layers):\n",
    "    \n",
    "    seed=30\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # nombre de layers dans le réseau\n",
    "    number_of_layers = len(layers)\n",
    "    # pour stocker les différentes valeurs des paramètres\n",
    "    \n",
    "    paramameters = {}\n",
    "    \n",
    "    # Pour toutes les couches du réseau\n",
    "    for idx, layer in enumerate(layers):\n",
    "        # Par simplification on commence la numérotation du layer à 1\n",
    "        # correspond aux données d'entrées (input), i.e. les variables prédictives\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # Initialisation des valeurs de la matrice W et du vecteur b\n",
    "        # pour les différentes couches\n",
    "        \n",
    "        paramameters['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        paramameters['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return paramameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des différentes fonctions d'activation (Relu, Sigmoid, Tanh) ainsi que les dérivées qui seront utilisées par la suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)# pour ne pas effacer dA\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "\n",
    "def tanh_backward (dA, Z):\n",
    "     return 1- dA**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante réalise la forward propagation mais uniquement d'un layer. Elle effectue donc le produit matriciel avec ajout du biais pour obtenir $Z$. Elle applique ensuite la fonction d'activation du layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \n",
    "    # regression linéaire sur les entrées du layer\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection de la fonction d'activation à utiliser dans la couche\n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    elif activation == \"tanh\":\n",
    "        activation_func = tanh\n",
    "        \n",
    "    # A est la sortie de la fonction d'activation    \n",
    "    A=activation_func(Z_curr)    \n",
    "    # retourne la fonction d'activation calculée et la matrice intermédiaire Z_curr\n",
    "    return A, Z_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante fait la forward propagation sur tout le réseau. Elle sauvegarde aussi les valeurs de $A$ et $Z$ dans un dictionnaire python indexé par ces lettres afin de pouvoir les retrouver facilement lors de l'étape de backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layers):\n",
    "    \n",
    "    # Création d'un cache temporaire qui contient les valeurs intermédiaires\n",
    "    # utiles lors de la phase de backward. Le fait de les sauvegarder permet\n",
    "    # de ne pas les recalculer lors du backward\n",
    "    cache = {}\n",
    "    \n",
    "    # A_curr correspond à la sortie du layer 0, i.e. les variables prédictives (input) \n",
    "    A_curr = X\n",
    "\n",
    "    # iteration pour l'ensemble des couches du réseau\n",
    "    for idx, layer in enumerate(layers):\n",
    "        # La numérotation des couches commence à 1 (0 pour la couche des données)\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # Recupération de l'activation de l'itération précédente\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # Récupération du nom de la fonction d'activation du layer courant\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        # Récupération du W et du b du layer courant \n",
    "        W_curr = parameters[\"W\" + str(layer_idx)]\n",
    "        b_curr = parameters[\"b\" + str(layer_idx)]\n",
    "\n",
    "        \n",
    "        # calcul de la fonction d'activation pour le layer courant\n",
    "        A_curr, Z_curr = one_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # Sauvegarde dans le cache pour la phase de backward\n",
    "        cache[\"A\" + str(idx)] = A_prev\n",
    "        cache[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # retourne le vecteur de prédiction à la sortie du réseau \n",
    "    # et un dictionnaire contenant toutes les valeurs intermédiaire \n",
    "    # pour faciliter la descente de gradient\n",
    "    \n",
    "    return A_curr, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le calcul de la fonction de coût (ici la cross entropy). y_hat correspond à la sortie du réseau après application de la forward propagation. y est la valeur réelle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_hat, y):\n",
    "    # calcul du coût (cross-entropy)\n",
    "    cost = (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux fonctions suivantes permettent de calculer l'accuracy du modèle. Elles suivent le même principe que celles vues dans le notebook descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_into_class(probs):\n",
    "    probs = np.copy(probs)#pour ne pas perdre probs\n",
    "    probs[probs > 0.5] = 1\n",
    "    probs[probs <= 0.5] = 0\n",
    "    return probs\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    y_hat_ = convert_prob_into_class(y_hat)\n",
    "    return (y_hat_ == y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation d'un niveau. Tout d'abord nous considérons la descente de gradient sur un niveau. Elle applique tout d'abord la dérivée de la fonction d'activation passée en paramètre, dW, db et donc la dérivée de Z pour un layer L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \n",
    "    # nombres d'exemples venant de la fonction d'activation précédente\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection de la fonction d'activation à appliquer\n",
    "    if activation ==\"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    elif activation == \"tanh\":\n",
    "        backward_activation_func = tanh_backward\n",
    "    \n",
    "    # calcul de la dérivée de la fonction d'activation \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # dérivée de la matrice W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # dérivée du vecteur b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # dérivée de la matrice A_Prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propagation sur tout le réseau commence par calculer la dérivée de la fonction de coût :\n",
    "    $$\\mathbf{\\frac{\\partial L}{\\partial A}} = \\mathbf{\\left(\\frac{-y}{A}+\\frac{(1-y)}{(1-A)}\\right)}$$  ou $$\\mathbf{-\\left(\\frac{y}{A}-\\frac{(1-y)}{(1-A)}\\right)}$$\n",
    "    et boucle sur les autres niveaux. Le cache est utilisé pour récupérer les valeurs de A et de Z en fonction de leur layer et ce cache est utilisé pour sauvegarder dW et db qui seront utilisés lors de la descente de gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(y_hat, y, cache, parameters, layers):\n",
    "    \n",
    "    \n",
    "    # Création d'un cache temporaire qui contient les dérivées (gradients) pour \n",
    "    # les différentes couches. Il est utilisé pour mettre à jour les paramètres\n",
    "    derivatives = {}\n",
    "    \n",
    "    # nombre d'exemples\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    # pour garantir que y a la même forme que y_hat\n",
    "    y = y.reshape(y_hat.shape)\n",
    "    \n",
    "    #  Initialisation du calcul de la dérivée de la fonction de coût\n",
    "    # par rapport à A pour la couche L \n",
    "    dA_prev = - (np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n",
    "    \n",
    "    # parcours du réseau de la fonction finale vers celle d'entrée\n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(layers))):\n",
    "        # Comme précédemment la numérotation des couches commence\n",
    "        # à 1. On ne modifie donc pas la couche 0\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        \n",
    "        # Récupération du nom de la fonction d'activation du layer courant\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        # Récupération dans le cache de la sortie précédente (A_prev)\n",
    "        # et de la matrice Z correspondant à l'application de la\n",
    "        # regression linéaire du niveau courant. Ceci évite de les recalculer.\n",
    "        A_prev = cache[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = cache[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        # Récupération dans parameters des valeurs de paramètres W et b du layer courant \n",
    "        W_curr = parameters[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = parameters[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        #application de la backwart propagation pour le layer afin d'avoir\n",
    "        #la valeur des dérivées (gradients)\n",
    "        dA_prev, dW_curr, db_curr = one_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        # sauvegarde pour mettre à jour les paramètres\n",
    "        derivatives[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        derivatives[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application de la descente de gradient pour mettre à jour les paramètres. Comme tous les gradients ont été calculés précédemment (et sauvegardés dans un dictionnaire), il suffit de les appliquer. Ici la descente est faite à la manière d'une descente par lot (*batch gradient descent*) et peut être facilement modifiée en mini-batch gradient descent (C.f. notebook descente de gradient) avec optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, derivatives, layers, eta):\n",
    "\n",
    "    # Mise à jour des paramètres sur les différentes couches \n",
    "    for layer_idx, layer in enumerate(layers, 1):\n",
    "        parameters[\"W\" + str(layer_idx)] -= eta * derivatives[\"dW\" + str(layer_idx)]        \n",
    "        parameters[\"b\" + str(layer_idx)] -= eta * derivatives[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction train permet de lancer les différentes phases en fonction du nombre d'epochs. Elle retourne l'historique du coût et de l'accuracy pour pouvoir les afficher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, layers, epochs, eta):\n",
    "    \n",
    "    # Initialisation des paramètres du réseau de neurones\n",
    "    parameters = init_layers(layers)\n",
    "    \n",
    "    # sauvegarde historique coût et accuracy pour affichage\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # Descente de gradient\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # forward progragation\n",
    "        y_hat, cache = forward_propagation(X, parameters, layers)\n",
    "        \n",
    "        # backward propagation - calcul des gradients\n",
    "        derivatives = backward_propagation(y_hat, y, cache, parameters, layers)\n",
    "        \n",
    "        # Mise à jour des paramètres\n",
    "        parameters = update(parameters, derivatives, layers, eta)\n",
    "        \n",
    "        # sauvegarde des historiques\n",
    "        current_cost=cost_function(y_hat, y)\n",
    "        cost_history.append(current_cost)\n",
    "        curent_accuracy = accuracy(y_hat, y)\n",
    "        accuracy_history.append(curent_accuracy) \n",
    "        \n",
    "        if(i % 100 == 0):\n",
    "            print(\"Epoch : #%s - cost : %.3f - accuracy : %.3f\"%(i, float(current_cost), curent_accuracy))\n",
    "        \n",
    "    return parameters,cost_history,accuracy_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier test avec la configuration de l'exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories (eta,epochs,cost_history,accuracy_history):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs),cost_history,label='Cost')\n",
    "    line2, = ax.plot(range(epochs),accuracy_history,label='Accuracy')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    " \n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "# sauvegarde pour comparaison avec Keras\n",
    "X_train_init=X_train\n",
    "X_test_init=X_test\n",
    "y_train_init=y_train\n",
    "y_test_init=y_test\n",
    "\n",
    "#transformation des données pour être au bon format\n",
    "X_train=np.transpose(X_train)\n",
    "X_test=np.transpose(X_test)\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], 1)))\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 600\n",
    "eta = 0.1\n",
    "\n",
    "parameters,cost_history,accuracy_history = fit(X_train, \n",
    "                                   y_train, \n",
    "                                  layers, epochs, eta)\n",
    "\n",
    "\n",
    "# Calcul de l'accuracy sur le jeu de test\n",
    "y_test_hat,_= forward_propagation(X_test, parameters, layers)\n",
    "accuracy_test = accuracy(y_test_hat, y_test)\n",
    "print(\"Accuracy : %.3f\"%accuracy_test)\n",
    "\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test en modifiant le réseau pour voir si le résultat est meilleur. Attention au surapprentissage dans ce cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 25, \"activation\": \"relu\"},    \n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "epochs = 600\n",
    "eta = 0.1\n",
    "parameters,cost_history,accuracy_history = fit(X_train, \n",
    "                                   y_train, \n",
    "                                  layers, epochs, eta)\n",
    "\n",
    "# Calcul de l'accuracy sur le jeu de test\n",
    "y_test_hat,_= forward_propagation(X_test, parameters, layers)\n",
    "accuracy_test = accuracy(y_test_hat, y_test)\n",
    "print(\"Accuracy : %.3f\"%accuracy_test)\n",
    "\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai avec Keras pour voir les résulats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction du modèle \n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2,activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "epochs = 600\n",
    "eta = 0.1\n",
    "\n",
    "gd = SGD(lr=eta)\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train_init, y_train_init, epochs=600, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model.predict_classes(X_test_init)\n",
    "accuracy_test = accuracy_score(y_test_init, y_test_hat)\n",
    "print(\"Accuracy pour Keras : %.3f\"%accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Implémentation avec une classe simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dans un premier temps, nous créons une classe simple qui reprend les fonctions précédentes et dans laquelle nous ajoutons simplement les fonctions d'activation Relu, LeakyRelu, tanh et sigmoid. Cette classe permet de faire de la classification binaire.   \n",
    "\n",
    "Récupération des librairies utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# importations utiles\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0) # pour avoir des figures de même taille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fonctions d'affichage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_histories (eta,epochs,cost_history,accuracy_history):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs),cost_history,label='Cost')\n",
    "    line2, = ax.plot(range(epochs),accuracy_history,label='Accuracy')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=4)})\n",
    "    \n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fonctions d'activations et dérivées des fonctions : sigmoid, tanh, relu et leakyrelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fonctions utiles\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01,x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x<=0] = 0.01\n",
    "    x[x>0] = 1\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Définition de la classe MyNeuralNetwork.   \n",
    "\n",
    "L'initialisation de la classe nécessite un réseau défini de la manière suivante :   \n",
    "layers = [  \n",
    "    {\"input_dim\": *nombre de neurones en entrée*, \"output_dim\": *nombre de neurones du layer*, \"activation\": *fonction d'activation du layer*},  \n",
    "    {\"input_dim\": *nombre de neurones layer précédent*, \"output_dim\": *nombre de neurones du layer*, \"activation\": *fonction d'activation du layer*},  \n",
    "    ...  \n",
    "    {\"input_dim\": *nombre de neurones layer précédent*, \"output_dim\": 1, \"activation\": *fonction d'activation de l'ouput layer*}  \n",
    "]  \n",
    "\n",
    "Pour l'exemple du notebook :   \n",
    "layers = [  \n",
    "    {\"input_dim\": 2, \"output_dim\": 3, \"activation\": \"relu\"},  \n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}  \n",
    "]  \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "La classe est assez similaire à l'appel des différentes fonctions. Ici dans le constructeur le réseau est construit. La principale différence est dans les fonctions forward et backward qui traitent différemment le cas des fonctions d'activations. Ici la descente de gradient se fait par mini-batch : d'où appel à la fonction *next_batch* vue dans le notebook descente de gradient qui retourne un batch de la taille de batchsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        seed=30\n",
    "        np.random.seed(seed)\n",
    "        self.L = len(layers) #L est la couche de sortie du réseau\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        \n",
    "        # Pour toutes les couches du réseau\n",
    "        for idx, layer in enumerate(layers):\n",
    "            \n",
    "            # Par simplification on commence la numérotation du layer à 1\n",
    "            # correspond aux données d'entrées (input), i.e. les variables prédictives\n",
    "            layer_idx = idx + 1\n",
    "        \n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "            # Initialisation des valeurs de la matrice W et du vecteur b\n",
    "            # pour les différentes couches\n",
    "            # W est initialisé en prenant l'optimisation de He et al 2015\n",
    "            self.parameters['W' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, layer_input_size) * np.sqrt(2/layer_input_size)\n",
    "            self.parameters['b' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, 1) * 0.1\n",
    "            \n",
    "            # Sauvegarde de la fonction d'activation du réseau\n",
    "            self.parameters['activation'+ str(layer_idx)]=layer[\"activation\"]\n",
    "\n",
    "        \n",
    "                    \n",
    "    def forward_propagation(self, X):\n",
    "        #Initialisation des variables prédictives pour la couche d'entrée\n",
    "        self.parameters['A0'] = X\n",
    "        \n",
    "        #Propagation pour tous les layers\n",
    "        for l in range(1, self.L + 1):\n",
    "            \n",
    "            # Calcul de Z\n",
    "            self.parameters['Z' + str(l)] = np.dot(self.parameters['W' + str(l)], \n",
    "                                            self.parameters['A' + str(l - 1)])+self.parameters['b' + str(l)]\n",
    "\n",
    "           \n",
    "            # Récupération de la fonction d'activation du layer\n",
    "            activ_function_curr = self.parameters['activation' + str(l)]\n",
    "            if activ_function_curr == \"relu\":\n",
    "                activation_func = relu\n",
    "            elif activ_function_curr == \"sigmoid\":\n",
    "                activation_func = sigmoid\n",
    "            elif activ_function_curr == \"tanh\":\n",
    "                activation_func = tanh\n",
    "            elif activ_function_curr == \"leakyrelu\":\n",
    "                activation_func = leakyrelu\n",
    "                \n",
    "            # Application de la fonction d'activation à Z    \n",
    "            self.parameters['A' + str(l)] = activation_func(self.parameters['Z' + str(l)])\n",
    "         \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "    \n",
    "    def accuracy(self,y_hat, y):\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()\n",
    "    \n",
    "    \n",
    "    def cost_function(self, y):\n",
    "        # cross entropy\n",
    "        return (-(y*np.log(self.parameters['A' + str(self.L)]+ 1e-8) + (1-y)*np.log( 1 - self.parameters['A' + str(self.L)]+ 1e-8))).mean()\n",
    "    \n",
    " \n",
    "    def backward_propagation(self, y):\n",
    "        #Dérivées partielles de la fonction de coût pour z[L], W[L] et b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dZ' + str(self.L)] = self.parameters['A' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dZ' + str(self.L)],\n",
    "                                                      np.transpose(self.parameters['A' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        #Attention pour un lot de m exemples, il faut faire la moyenne des dérivées sur les m exemples\n",
    "        m = self.parameters['A' + str(self.L)].shape[1]\n",
    "        self.derivatives['db' + str(self.L)]  = np.sum(self.derivatives['dZ' + str(self.L)], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "\n",
    "        #Dérivées partielles de la fonction de coût pour z[l], W[l] et b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            \n",
    "            #Récupération de la dérivée de la fonction d'activation\n",
    "            activ_function_backward = self.parameters['activation' + str(l)]\n",
    "            if activ_function_backward == \"relu\":\n",
    "                backward_activation_func = relu_prime\n",
    "            elif activ_function_backward == \"sigmoid\":\n",
    "                backward_activation_func = sigmoid_prime\n",
    "            elif activ_function_backward == \"tanh\":\n",
    "                backward_activation_func = tanh_prime\n",
    "            elif activ_function_backward == \"leakyrelu\":\n",
    "                backward_activation_func = leakyrelu_prime\n",
    "                \n",
    "            self.derivatives['dZ' + str(l)] = np.dot(np.transpose(self.parameters['W' + str(l + 1)]),\n",
    "                self.derivatives['dZ' + str(l + 1)])*backward_activation_func(self.parameters['Z' + str(l)])\n",
    "\n",
    "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dZ' + str(l)], \n",
    "                np.transpose(self.parameters['A' + str(l - 1)]))\n",
    "            #Attention pour un lot de m exemples, il faut faire la moyenne des dérivées sur les m exemples\n",
    "            m = self.parameters['A' + str(l - 1)].shape[1]\n",
    "            self.derivatives['db' + str(l)]  = np.sum(self.derivatives['dZ' + str(l)], \n",
    "                                                      axis=1, keepdims=True) / m\n",
    "            \n",
    "    def update_parameters(self, eta):\n",
    "        # Descente de gradient\n",
    "        for l in range(1, self.L+1):    \n",
    "            self.parameters['W' + str(l)] -= eta*self.derivatives['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= eta*self.derivatives['db' + str(l)]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.parameters['A' + str(self.L)]\n",
    " \n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs, eta = 0.01,batchsize=64):\n",
    "        \n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        for i in range(epochs):\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for (batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch à la fois\n",
    "                \n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                \n",
    "                self.forward_propagation(batchX)\n",
    "                self.backward_propagation(batchy)\n",
    "                self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "                \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(i % 10 == 0):\n",
    "                print(\"Epoch : #%s - cost : %.3f - accuracy : %.3f\"%(i, float(current_cost), current_accuracy))\n",
    "        return self.parameters, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Utilisation de la classe sur un jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=cm_bright)#cmap=plt.cm.PiYG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Création d'un jeu de données d'apprentissage et de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#création d'un jeu d'apprentissage et de test\n",
    "\n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Attention, les variables prédictives $X$ doivent être sous la forme $m$ colonnes où $m$ est le nombre d'exemples d'apprentissage et $l$ lignes où $l$ est le nombre de variables prédictives. Dans notre exemple du notebook, il y a deux variables prédictives donc il faut passer que $X$ soit de la forme $m$ colonnes et 2 lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#transformation des données pour être au bon format\n",
    "# X_train est de la forme : 2 colonnes, m lignes (examples)\n",
    "# y_train est de la forme : m colonnes, 1 ligne\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), 2 lignes\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], 1)))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], 1)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Création du réseau, entraînement du classifieur et prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# le réseau du notebook\n",
    "layers = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "epochs = 100\n",
    "eta = 0.01\n",
    "batchsize=32\n",
    "\n",
    "#Création du classifieur (mise en place des layers et initialisation des W et b)\n",
    "# optimizer = bgd (descente par mini-batch) par défaut sinon optimizer=\"momentum\"/\"adam\"\n",
    "clf = MyNeuralNetwork(layers)\n",
    "\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta,batchsize)\n",
    "\n",
    "#Prédiction\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test: %.3f\"%accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Affichage de l'historique de la fonction de coût et de l'accuracy ainsi que de la frontière de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)\n",
    "#plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En changeant la configuration du réseau. Attention au surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 3, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "clf = MyNeuralNetwork(layers)\n",
    "\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy : %.3f\"%accuracy_test)\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classe avec optimisation (momentum, adam) pour faire de la classification multi-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous présentons une classe plus complète qui permet de faire de la classification multi-classes, i.e. via la fonction d'activation softmax. En outre, elle propose de pouvoir utiliser des optimisations : momentum et adam.   \n",
    "\n",
    "La structure de la classe est assez similaire à la précédente.  \n",
    "La fonction predict est changée pour pouvoir prédire la classe d'appartenance lors de multi-classes.  \n",
    "La fonction update prend en compte le fait que l'optimisation peut être de type momentum ou adam.  \n",
    "Le constructeur prend par défaut *optimizer=\"bgd\"* (*batch gradient descent*) (les valeurs possibles sont *\"momentum\"* ou *\"adam\"*.\n",
    "Enfin lors de l'appel de la fonction fit les paramètres beta, beta1 et epsilon doivent être spécifiées (en cas d'appel d'un optimizer. Ici il y a des valeurs par défaut : beta=0.9,beta2=0.999, epsilon=1e-8.\n",
    "\n",
    "La construction du réseau est similaire à celui de la classe précédente :  \n",
    "\n",
    "Pour l'exemple du notebook :   \n",
    "layers = [  \n",
    "    {\"input_dim\": 2, \"output_dim\": 3, \"activation\": \"relu\"},  \n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"}  \n",
    "]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importations utiles\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0) # pour avoir des figures de même taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonctions utiles\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01,x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x<=0] = 0.01\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, layers,optimizer=\"bgd\"):\n",
    "        seed=30\n",
    "        np.random.seed(seed)\n",
    "        self.L = len(layers) #L est la couche de sortie du réseau\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        self.optimizer=optimizer\n",
    "        if optimizer==\"momentum\":\n",
    "            self.v={} # vitesse pour momentum\n",
    "        elif optimizer == \"adam\":\n",
    "            self.v={}\n",
    "            self.s={}\n",
    "            self.t=2\n",
    "            \n",
    "    \n",
    "        # Pour toutes les couches du réseau\n",
    "        for idx, layer in enumerate(layers):\n",
    "            \n",
    "            # Par simplification on commence la numérotation du layer à 1\n",
    "            # correspond aux données d'entrées (input), i.e. les variables prédictives\n",
    "            layer_idx = idx + 1\n",
    "        \n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "            # Initialisation des valeurs de la matrice W et du vecteur b\n",
    "            # pour les différentes couches\n",
    "            # W est initialisé en prenant l'optimisation de He et al 2015\n",
    "            self.parameters['W' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, layer_input_size) * np.sqrt(2/layer_input_size)\n",
    "            self.parameters['b' + str(layer_idx)] = np.random.randn(\n",
    "                layer_output_size, 1) * 0.1\n",
    "            \n",
    "            # Sauvegarde de la fonction d'activation du réseau\n",
    "            self.parameters['activation'+ str(layer_idx)]=layer[\"activation\"]\n",
    "            if optimizer==\"momentum\":\n",
    "                # Sauvegarde velocity, v, pour momentum\n",
    "                self.v['dW' + str(layer_idx)] = np.zeros_like(self.parameters['W' + str(layer_idx)])\n",
    "                self.v['db' + str(layer_idx)] = np.zeros_like(self.parameters['b' + str(layer_idx)])\n",
    "            elif optimizer=='adam':    \n",
    "                self.v['dW' + str(layer_idx)] = np.zeros_like(self.parameters['W' + str(layer_idx)])\n",
    "                self.v['db' + str(layer_idx)] = np.zeros_like(self.parameters['b' + str(layer_idx)])\n",
    "                self.s['dW' + str(layer_idx)] = np.zeros_like(self.parameters['W' + str(layer_idx)])\n",
    "                self.s['db' + str(layer_idx)] = np.zeros_like(self.parameters['b' + str(layer_idx)])\n",
    "        \n",
    "                    \n",
    "    def forward_propagation(self, X):\n",
    "        #Initialisation des variables prédictives pour la couche d'entrée\n",
    "        self.parameters['A0'] = X\n",
    "        \n",
    "        #Propagation pour tous les layers\n",
    "        for l in range(1, self.L + 1):\n",
    "            \n",
    "            # Calcul de Z\n",
    "            self.parameters['Z' + str(l)] = np.dot(self.parameters['W' + str(l)], \n",
    "                                            self.parameters['A' + str(l - 1)])+self.parameters['b' + str(l)]\n",
    "           \n",
    "            # Récupération de la fonction d'activation du layer\n",
    "            activ_function_curr = self.parameters['activation' + str(l)]\n",
    "            if activ_function_curr == \"relu\":\n",
    "                activation_func = relu\n",
    "            elif activ_function_curr == \"sigmoid\":\n",
    "                activation_func = sigmoid\n",
    "            elif activ_function_curr == \"tanh\":\n",
    "                activation_func = tanh\n",
    "            elif activ_function_curr == \"leakyrelu\":\n",
    "                activation_func = leakyrelu\n",
    "            elif activ_function_curr == \"softmax\":\n",
    "                activation_func = softmax\n",
    "                \n",
    "            # Application de la fonction d'activation à Z    \n",
    "            self.parameters['A' + str(l)] = activation_func(self.parameters['Z' + str(l)])\n",
    "\n",
    "            \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    " \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.parameters['activation' + str(self.L)]==\"softmax\":\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()\n",
    "    \n",
    "    \n",
    "    def cost_function(self, y):\n",
    "        # cross entropy\n",
    "        return (-(y*np.log(self.parameters['A' + str(self.L)]+1e-8) + (1-y)*np.log( 1 - self.parameters['A' + str(self.L)]+1e-8))).mean()\n",
    "    \n",
    " \n",
    "    def backward_propagation(self, y):\n",
    "        #Dérivées partielles de la fonction de coût pour z[L], W[L] et b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dZ' + str(self.L)] = self.parameters['A' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dZ' + str(self.L)],\n",
    "                                                      np.transpose(self.parameters['A' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        #Attention pour un lot de m exemples, il faut faire la moyenne des dérivées sur les m exemples\n",
    "        m = self.parameters['A' + str(self.L)].shape[1]\n",
    "        self.derivatives['db' + str(self.L)]  = np.sum(self.derivatives['dZ' + str(self.L)], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "\n",
    "        #Dérivées partielles de la fonction de coût pour z[l], W[l] et b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            \n",
    "            #Récupération de la dérivée de la fonction d'activation\n",
    "            activ_function_backward = self.parameters['activation' + str(l)]\n",
    "            if activ_function_backward == \"relu\":\n",
    "                backward_activation_func = relu_prime\n",
    "            elif activ_function_backward == \"sigmoid\":\n",
    "                backward_activation_func = sigmoid_prime\n",
    "            elif activ_function_backward == \"tanh\":\n",
    "                backward_activation_func = tanh_prime\n",
    "            elif activ_function_backward == \"leakyrelu\":\n",
    "                backward_activation_func = leakyrelu_prime\n",
    "                    \n",
    "            self.derivatives['dZ' + str(l)] = np.dot(np.transpose(self.parameters['W' + str(l + 1)]),\n",
    "                self.derivatives['dZ' + str(l + 1)])*backward_activation_func(self.parameters['Z' + str(l)])\n",
    "\n",
    "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dZ' + str(l)], \n",
    "                np.transpose(self.parameters['A' + str(l - 1)]))\n",
    "            #Attention pour un lot de m exemples, il faut faire la moyenne des dérivées sur les m exemples\n",
    "            m = self.parameters['A' + str(l - 1)].shape[1]\n",
    "            self.derivatives['db' + str(l)]  = np.sum(self.derivatives['dZ' + str(l)], \n",
    "                                                      axis=1, keepdims=True) / m\n",
    "            \n",
    "    def update_parameters(self, eta,beta, beta2=0.999, epsilon=1e-8):\n",
    "        # Descente de gradient\n",
    "        if self.optimizer==\"adam\":\n",
    "            v_corrected = {}            # Initialisation de la première estimation du moment\n",
    "            s_corrected = {}            # Initialisation de la seconde estimation du moment\n",
    "            \n",
    "        for l in range(1, self.L+1):    \n",
    "            if self.optimizer==\"momentum\":\n",
    "                # Calcul de la vitesse \n",
    "                self.v['dW' + str(l)] = beta * self.v['dW' + str(l)] + (1 - beta) * self.derivatives['dW' + str(l)]\n",
    "                self.v['db' + str(l)] = beta * self.v['db' + str(l)] + (1 - beta) * self.derivatives['db' + str(l)]\n",
    "            \n",
    "                # Mise à jour des paramètres\n",
    "                self.parameters['W' + str(l)] -= eta*self.v['dW' + str(l)]\n",
    "                self.parameters['b' + str(l)] -= eta*self.v['db' + str(l)]\n",
    "            elif self.optimizer==\"adam\":  \n",
    "                # Calcul de la vitesse \n",
    "                self.v['dW' + str(l)] = beta * self.v['dW' + str(l)] + (1 - beta) * self.derivatives['dW' + str(l)]\n",
    "                self.v['db' + str(l)] = beta * self.v['db' + str(l)] + (1 - beta) * self.derivatives['db' + str(l)]\n",
    "\n",
    "                # Calcul de la première estimation du moment (correction du biais)\n",
    "                v_corrected[\"dW\" + str(l)] = self.v[\"dW\" + str(l)] / (1 - np.power(beta, self.t))\n",
    "                v_corrected[\"db\" + str(l)] = self.v[\"db\" + str(l)] / (1 - np.power(beta, self.t))\n",
    "\n",
    "                 # déplacement moyen des gradients au carré\n",
    "                self.s[\"dW\" + str(l)] = beta2 * self.s[\"dW\" + str(l)] + (1 - beta2) * np.power(self.derivatives['dW' + str(l)], 2)\n",
    "                self.s[\"db\" + str(l)] = beta2 * self.s[\"db\" + str(l)] + (1 - beta2) * np.power(self.derivatives['db' + str(l)], 2)\n",
    "                \n",
    "                # Calcul de la seconde estimation du moment (correction du biais)\n",
    "                s_corrected[\"dW\" + str(l)] = self.s[\"dW\" + str(l)] / (1 - np.power(beta2, self.t))\n",
    "                s_corrected[\"db\" + str(l)] = self.s[\"db\" + str(l)] / (1 - np.power(beta2, self.t))\n",
    "                \n",
    "                # Mise à jour des paramètres\n",
    "                self.parameters['W' + str(l)] -= eta*v_corrected[\"dW\" + str(l)] / np.sqrt(s_corrected[\"dW\" + str(l)] + epsilon)\n",
    "                self.parameters['b' + str(l)] -= eta*v_corrected[\"db\" + str(l)] / np.sqrt(s_corrected[\"db\" + str(l)] + epsilon)\n",
    "               \n",
    "            else: #descente par mini-lots\n",
    "                self.parameters['W' + str(l)] -= eta*self.derivatives['dW' + str(l)]\n",
    "                self.parameters['b' + str(l)] -= eta*self.derivatives['db' + str(l)]\n",
    "            \n",
    "       \n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.parameters['A' + str(self.L)]\n",
    "    \n",
    " \n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs, eta = 0.01,batchsize=32,beta=0.9,beta2=0.999, epsilon=1e-8):\n",
    "        \n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        for i in range(epochs):\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for (batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch à la fois\n",
    "                \n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.parameters['activation' + str(self.L)]==\"softmax\":\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe\n",
    "                    # pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                \n",
    "                \n",
    "                self.forward_propagation(batchX)\n",
    "                self.backward_propagation(batchy)\n",
    "                if self.optimizer==\"adam\":\n",
    "                    self.t=self.t+1\n",
    "                self.update_parameters(eta,beta,beta2,epsilon)\n",
    "                #else:\n",
    "                #    self.update_parameters(eta,0)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "                \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(i % 10 == 0):\n",
    "                print(\"Epoch : #%s - cost : %.3f - accuracy : %.3f\"%(i, float(current_cost), current_accuracy))\n",
    "        return self.parameters, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essai classe avec optimisation sur différents jeux de données - classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=cm_bright)#cmap=plt.cm.PiYG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même en utilisant l'optimisation Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Avec Momentum\")\n",
    "#Création du classifieur (mise en place des layers et initialisation des W et b)\n",
    "optimizer=\"momentum\"\n",
    "clf = MyNeuralNetwork(layers, optimizer)\n",
    "beta=0.9\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy : %.3f\"%accuracy_test)\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même avec adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Avec Adam\")\n",
    "#Création du classifieur (mise en place des layers et initialisation des W et b)\n",
    "optimizer=\"adam\"\n",
    "clf = MyNeuralNetwork(layers, optimizer)\n",
    "beta=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-8\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test : %.3f\"%accuracy_test)\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur un autre jeu de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, noise=0.05, factor=0.7, random_state=0)\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=cm_bright)#cmap=plt.cm.PiYG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "#transformation des données pour être au bon format\n",
    "# X_train est de la forme : 2 colonnes, m lignes (examples)\n",
    "# y_train est de la forme : m colonnes, 1 ligne\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), 2 lignes\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], 1)))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], 1)))\n",
    "\n",
    "\n",
    "# déclaration de l'architecture du réseau \n",
    "layers = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"}, \n",
    "    {\"input_dim\": 25, \"output_dim\": 3, \"activation\": \"relu\"},     \n",
    "    {\"input_dim\": 3, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "eta = 0.01\n",
    "beta=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-8\n",
    "#Création du classifieur (mise en place des layers et initialisation des W et b)\n",
    "print (\"Batch gradient descent\")\n",
    "clf = MyNeuralNetwork(layers)\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "print (\"\\nBatch gradient descent et momentum\")\n",
    "clf = MyNeuralNetwork(layers,optimizer=\"momentum\")\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "print (\"\\nBatch gradient descent et adam\")\n",
    "clf = MyNeuralNetwork(layers,optimizer=\"momentum\")\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta)\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "\n",
    "#Prédiction \n",
    "y_pred= clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test : %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage de la frontière de décision\n",
    "plot_decision_boundary(lambda x: clf.predict(np.transpose(x)), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essai classe avec classification multi-classes (softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous illustrons l'utilisation de la fonction d'activation softmax pour faire de la classification multi-classes. Nous reprenons l'exemple d'IRIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "df = pd.read_csv(url, names=names)\n",
    "\n",
    "# mélange des données\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "array = df.values #necessité de convertir le dataframe en numpy\n",
    "#X matrice de variables prédictives - attention forcer le type à float\n",
    "X = array[:,0:4].astype('float32') \n",
    "#y vecteur de variable à prédire \n",
    "y = array[:,4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Première étape : normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation de X\n",
    "sc_X=StandardScaler()\n",
    "X=sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seconde étape : transformation des variables prédites à l'aide de OneHotEncoder : mettre 1 colonne par classe. Quand un exemple d'apprentissage correspond à la classe, il y a un 1 à la ligne et la colonne correspondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la variable à prédire via OneHotEncoder \n",
    "# Dans IRIS il y a 3 classes -> création de 3 colonnes pour y\n",
    "# 1 colonne correspond à 1 classe -> 1 si la ligne est du type de la classe\n",
    "# 0 sinon\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment création du jeu d'apprentissage et de test. Attention, il faut mettre les variables prédictives et prédites au bon format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeu de test/apprentissage\n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "#transformation des données pour être au bon format\n",
    "# X_train est de la forme : n colonnes (variables à prédire après OneHotEncoder), m lignes (examples)\n",
    "# y_train est de la forme : m colonnes, n lignes (variables à prédire après OneHotEncoder)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], y_train.shape[1])))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], y_test.shape[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du réseau et lancement du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# déclaration de l'architecture du réseau \n",
    "layers = [\n",
    "    {\"input_dim\": 4, \"output_dim\": 10, \"activation\": \"leakyrelu\"},\n",
    "    {\"input_dim\": 10, \"output_dim\": 3, \"activation\": \"softmax\"},\n",
    "]\n",
    "\n",
    "\n",
    "#Création du classifieur (mise en place des layers et initialisation des W et b)\n",
    "# optimizer = bgd (descente par mini-batch) par défaut sinon optimizer=\"momentum\"/\"adam\"\n",
    "clf = MyNeuralNetwork(layers,optimizer=\"adam\")\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.01\n",
    "batchsize=10\n",
    "beta=0.9\n",
    "#Entraînement du classifieur\n",
    "parameters,cost_history,accuracy_history=clf.fit(X_train, y_train, epochs,eta,batchsize,beta)\n",
    "\n",
    "#Prédiction \n",
    "y_pred= clf.predict(X_test)\n",
    "accuracy_test = clf.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "#Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
