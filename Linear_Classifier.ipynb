{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMIN339 : Méthodes Avncées de la Science de données\n",
    "\n",
    "\n",
    "## **`Réconnaissance Visuelle de Plantes`**\n",
    "\n",
    "### Object du Project :\n",
    "Réconnaissance d'espèces de plantes à partir de photos\n",
    "\n",
    "### Jeu de Départ : \n",
    "3474 images appartenant à 50 espèces différentes\n",
    "\n",
    "### Encadrement :\n",
    "* **`Konstantin TODOROV`**\n",
    "* **`Pascal PONCELET`**\n",
    " \n",
    "### Fait par :\n",
    "* **`BEYA NTUMBA Joel`**\n",
    "* **`MINKO AMOA Dareine`**\n",
    "* **`QUENETTE Christophe`**\n",
    "* **`SHAQURA Tasnim`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifcation of the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1737 images for Class 0, 1737 for Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.zeros(1737)\n",
    "y1 = np.ones(1737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate y0 and y1 to form y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed images trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3474, 150, 150, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((3474, 150, 150, 3))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r preprocessed_images\n",
    "X, y = preprocessed_images\n",
    "X = X.astype('float32')\n",
    "y = np.concatenate((y0, y1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2084, 150, 150, 3)\n",
      "X_test: (209, 150, 150, 3)\n",
      "X_val: (417, 150, 150, 3)\n",
      "y_train: (2084,)\n",
      "y_test: (209,)\n",
      "y_val: (417,)\n"
     ]
    }
   ],
   "source": [
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "# testsize_train= 1-validation_size\n",
    "# testsize = 1 - validation_size\n",
    "seed=42\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.20)\n",
    "\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_test, \n",
    "                                               y_test, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.3)\n",
    "\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"X_val: \" + str(X_val.shape))\n",
    "print(\"y_train: \" + str(y_train.shape))\n",
    "print(\"y_test: \" + str(y_test.shape))\n",
    "print(\"y_val: \" + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming X_test, X_train, y_train, y_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = X_train.shape[0]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "num_test = X_test.shape[0]\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "num_val = X_val.shape[0]\n",
    "mask = list(range(num_val))\n",
    "X_val = X_val[mask]\n",
    "y_val = y_val[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the image data into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 67500) (209, 67500) (417, 67500)\n",
      "(2084,) (209,) (417,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data to zero mean, i.e centred around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_val -= mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the biais dimension of ones (i.e. biais trick) so that our SVM only has to worry about optimizing a single weight matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 67501) (209, 67501) (417, 67501)\n",
      "Data ready\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(\"Data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "X_train, X_test and X_val are centred around zero by subtracting the mean from the sets. A zero mean is a very common data pre-processing technique as it ensures that the gradients calculated remain controlled and to increase robustness to noise. It is important to note that the mean should be calculated only using the training data and not the validation or test sets. The biases are appended to weights as the last column of the weight matrix, so that now only one matrix W is to be optimized. This is also called the bias trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    y = [int(x) for x in y]\n",
    "    correct_class_scores = scores[np.arange(num_train), y].reshape(num_train, 1)\n",
    "    margin = np.maximum(0, scores - correct_class_scores + 1)\n",
    "    margin[np.arange(num_train), y] = 0  # do not consider correct class in loss\n",
    "    loss = margin.sum() / num_train\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "\n",
    "    margin[margin > 0] = 1\n",
    "    valid_margin = margin.sum(axis=1)\n",
    "    margin[np.arange(num_train), y] -= valid_margin\n",
    "    dW = (X.T).dot(margin) / num_train\n",
    "    # Regularization gradient\n",
    "    dW = dW + reg * 2 * W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "The svm_loss_vectorized function has the arguments W matrix that consists of the weights as well as the bias,input matrix X, target matrix y and reg the regularization strength. The scores metric is calculated according to W.X + b, which is reduced to W.X as the biases are included in matrix W with the bias trick. The loss is calculated from the average difference between the true target matrix y and the predicted scores. A further L2 regularization loss is added to encourage the weights to stay low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import object\n",
    "\n",
    "class LinearClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = float(np.max(y)) + 1.0 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, int(num_classes))\n",
    "        \n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            batch_indices = np.random.choice(num_train, batch_size, replace=False)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch=y[batch_indices]\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "\n",
    "            self.W -= learning_rate*grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = scores.argmax(axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:** ##\n",
    "The Linear Classifier class during training calculates the loss via the svm_loss_vectorized() function, which returns the loss as well as the gradient dw. The weights matrix is then updated with the equation W = W-a*dw. This process is repeated over several iterations called as epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 3384.370059\n",
      "iteration 100 / 1500: loss 1240.718282\n",
      "iteration 200 / 1500: loss 455.267525\n",
      "iteration 300 / 1500: loss 167.225650\n",
      "iteration 400 / 1500: loss 62.139692\n",
      "iteration 500 / 1500: loss 23.739515\n",
      "iteration 600 / 1500: loss 9.461212\n",
      "iteration 700 / 1500: loss 4.108287\n",
      "iteration 800 / 1500: loss 2.172159\n",
      "iteration 900 / 1500: loss 1.054002\n",
      "iteration 1000 / 1500: loss 1.514256\n",
      "iteration 1100 / 1500: loss 0.838581\n",
      "iteration 1200 / 1500: loss 1.421074\n",
      "iteration 1300 / 1500: loss 0.656138\n",
      "iteration 1400 / 1500: loss 1.201591\n",
      "training accuracy: 0.654031\n",
      "validation accuracy: 0.517986\n"
     ]
    }
   ],
   "source": [
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1500, verbose=True)\n",
    "\n",
    "y_train_pred = svmd.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred),))\n",
    "\n",
    "y_val_pred = svmd.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "An object of class LinearSVM is trained over X_train for 1500 epochs using some hyperparameters, learning rate and regularization strength. These hyperparameters will be tuned to achieve best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters to get the Best Fit\n",
    "\n",
    "#### Use the validation set to the hyperarameters (Regularizition strength and learning rate. You should experiment with different ranges for the learning rates and regularization strenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3]\n",
    "regularization_strengths = [2.5e4, 5e4, 2.5e3, 2.5e2, 5e2, 1e1]\n",
    "\n",
    "results = {}\n",
    "best_val = -1  # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None  # The LinearSVM object that achieved the highest validation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare blr, brg to store the best LinearSVM object's learning rate and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 5.000000e-07 reg 1.000000e+01 train accuracy: 0.996161 val accuracy: 0.494005\n",
      "lr 5.000000e-07 reg 2.500000e+02 train accuracy: 0.994242 val accuracy: 0.494005\n",
      "lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.994722 val accuracy: 0.503597\n",
      "lr 5.000000e-07 reg 2.500000e+03 train accuracy: 0.621881 val accuracy: 0.515588\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.541267 val accuracy: 0.529976\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.513916 val accuracy: 0.455635\n",
      "lr 5.000000e-06 reg 1.000000e+01 train accuracy: 0.995681 val accuracy: 0.448441\n",
      "lr 5.000000e-06 reg 2.500000e+02 train accuracy: 0.612764 val accuracy: 0.537170\n",
      "lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.657390 val accuracy: 0.532374\n",
      "lr 5.000000e-06 reg 2.500000e+03 train accuracy: 0.533109 val accuracy: 0.465228\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.499040 val accuracy: 0.458034\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.493282 val accuracy: 0.453237\n",
      "lr 5.000000e-05 reg 1.000000e+01 train accuracy: 0.826775 val accuracy: 0.470024\n",
      "lr 5.000000e-05 reg 2.500000e+02 train accuracy: 0.524952 val accuracy: 0.462830\n",
      "lr 5.000000e-05 reg 5.000000e+02 train accuracy: 0.509117 val accuracy: 0.467626\n",
      "lr 5.000000e-05 reg 2.500000e+03 train accuracy: 0.507198 val accuracy: 0.544365\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 1.000000e+01 train accuracy: 0.551823 val accuracy: 0.539568\n",
      "lr 5.000000e-04 reg 2.500000e+02 train accuracy: 0.511516 val accuracy: 0.546763\n",
      "lr 5.000000e-04 reg 5.000000e+02 train accuracy: 0.510557 val accuracy: 0.553957\n",
      "lr 5.000000e-04 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 1.000000e+01 train accuracy: 0.519194 val accuracy: 0.529976\n",
      "lr 5.000000e-03 reg 2.500000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "best validation accuracy achieved during cross-validation: 0.553957\n",
      "linear SVM on raw pixels final test set accuracy: 0.497608\n",
      "SVMD\n",
      "[1 0 0 1 1 1 0 1 0 1]\n",
      "actual\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "best_svm\n",
      "[1 0 0 1 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "blr = None\n",
    "brg = None\n",
    "\n",
    "grid_search = [(lr, rg) for lr in learning_rates for rg in regularization_strengths]\n",
    "for lr, rg in grid_search:\n",
    "    svmd = LinearSVM()\n",
    "    train_loss = svmd.train(X_train, y_train, learning_rate=lr, reg=rg,\n",
    "                            num_iters=2000, verbose=False)\n",
    "    # Predict values for training set\n",
    "    y_train_pred = svmd.predict(X_train)\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = np.mean(y_train_pred == y_train)\n",
    "    # Predict values for validation set\n",
    "    y_val_pred = svmd.predict(X_val)\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = np.mean(y_val_pred == y_val)\n",
    "    # Save results\n",
    "    results[(lr, rg)] = (train_accuracy, val_accuracy)\n",
    "    if best_val < val_accuracy:\n",
    "        blr = lr\n",
    "        brg = rg\n",
    "        best_val = val_accuracy\n",
    "        best_svm = svmd\n",
    "        \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "        lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)\n",
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=blr, reg=brg,\n",
    "                       num_iters=2000, verbose=False)\n",
    "y_svmd = svmd.predict(X_test)\n",
    "print(\"SVMD\")\n",
    "print(y_svmd[0:10])\n",
    "print(\"actual\")\n",
    "print(y_test[0:10])\n",
    "print(\"best_svm\")\n",
    "print(y_test_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996641074856046\n",
      "0.460431654676259\n",
      "0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "classifier = svm.LinearSVC(penalty='l2', loss='squared_hinge', max_iter=2000)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.score(X_train, y_train))\n",
    "print(classifier.score(X_val, y_val))\n",
    "print(classifier.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
