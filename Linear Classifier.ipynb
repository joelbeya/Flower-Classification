{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMIN339 : Méthodes Avncées de la Science de données\n",
    "\n",
    "\n",
    "## **`Réconnaissance Visuelle de Plantes`**\n",
    "\n",
    "### Object du Project :\n",
    "Réconnaissance d'espèces de plantes à partir de photos\n",
    "\n",
    "### Jeu de Départ : \n",
    "3474 images appartenant à 50 espèces différentes\n",
    "\n",
    "### Encadrement :\n",
    "* **`Konstantin TODOROV`**\n",
    "* **`Pascal PONCELET`**\n",
    " \n",
    "### Fait par :\n",
    "* **`BEYA NTUMBA Joel`**\n",
    "* **`MINKO AMOA Dareine`**\n",
    "* **`QUENETTE Christophe`**\n",
    "* **`SHAQURA Tasnim`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifcation of the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2000 images for Class 0, 2134 for Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.zeros(1737)\n",
    "y1 = np.ones(1737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate y0 and y1 to form y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed images trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r preprocessed_images\n",
    "X, y = preprocessed_images\n",
    "X = X.astype('float32')\n",
    "y = np.concatenate((y0, y1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2084, 150, 150, 3)\n",
      "X_test: (209, 150, 150, 3)\n",
      "X_val: (417, 150, 150, 3)\n",
      "y_train: (2084,)\n",
      "y_test: (209,)\n",
      "y_val: (417,)\n"
     ]
    }
   ],
   "source": [
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "# testsize_train= 1-validation_size\n",
    "# testsize = 1 - validation_size\n",
    "seed=42\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.20)\n",
    "\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_test, \n",
    "                                               y_test, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=0.3)\n",
    "\n",
    "print(\"X_train: \" + str(X_train.shape))\n",
    "print(\"X_test: \" + str(X_test.shape))\n",
    "print(\"X_val: \" + str(X_val.shape))\n",
    "print(\"y_train: \" + str(y_train.shape))\n",
    "print(\"y_test: \" + str(y_test.shape))\n",
    "print(\"y_val: \" + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming X_test, X_train, y_train, y_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = X_train.shape[0]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "num_test = X_test.shape[0]\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "num_val = X_val.shape[0]\n",
    "mask = list(range(num_val))\n",
    "X_val = X_val[mask]\n",
    "y_val = y_val[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the image data into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 67500) (209, 67500) (417, 67500)\n",
      "(2084,) (209,) (417,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data to zero mean, i.e centred around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_val -= mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the biais dimension of ones (i.e. biais trick) so that our SVM only has to worry about optimizing a single weight matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 67501) (209, 67501) (417, 67501)\n",
      "Data ready\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(\"Data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "X_train, X_test and X_val are centred around zero by subtracting the mean from the sets. A zero mean is a very common data pre-processing technique as it ensures that the gradients calculated remain controlled and to increase robustness to noise. It is important to note that the mean should be calculated only using the training data and not the validation or test sets. The biases are appended to weights as the last column of the weight matrix, so that now only one matrix W is to be optimized. This is also called the bias trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    scores = X.dot(W)\n",
    "    y = [int(x) for x in y]\n",
    "    correct_class_scores = scores[np.arange(num_train), y].reshape(num_train, 1)\n",
    "    margin = np.maximum(0, scores - correct_class_scores + 1)\n",
    "    margin[np.arange(num_train), y] = 0  # do not consider correct class in loss\n",
    "    loss = margin.sum() / num_train\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "\n",
    "    margin[margin > 0] = 1\n",
    "    valid_margin = margin.sum(axis=1)\n",
    "    margin[np.arange(num_train), y] -= valid_margin\n",
    "    dW = (X.T).dot(margin) / num_train\n",
    "    # Regularization gradient\n",
    "    dW = dW + reg * 2 * W\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "The svm_loss_vectorized function has the arguments W matrix that consists of the weights as well as the bias,input matrix X, target matrix y and reg the regularization strength. The scores metric is calculated according to W.X + b, which is reduced to W.X as the biases are included in matrix W with the bias trick. The loss is calculated from the average difference between the true target matrix y and the predicted scores. A further L2 regularization loss is added to encourage the weights to stay low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import object\n",
    "\n",
    "class LinearClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = float(np.max(y)) + 1.0 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(dim, int(num_classes))\n",
    "        \n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            batch_indices = np.random.choice(num_train, batch_size, replace=False)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch=y[batch_indices]\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "\n",
    "            self.W -= learning_rate*grad\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = scores.argmax(axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:** ##\n",
    "The Linear Classifier class during training calculates the loss via the svm_loss_vectorized() function, which returns the loss as well as the gradient dw. The weights matrix is then updated with the equation W = W-a*dw. This process is repeated over several iterations called as epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 3377.134514\n",
      "iteration 100 / 1500: loss 1238.766282\n",
      "iteration 200 / 1500: loss 454.501373\n",
      "iteration 300 / 1500: loss 166.742390\n",
      "iteration 400 / 1500: loss 61.896656\n",
      "iteration 500 / 1500: loss 23.400779\n",
      "iteration 600 / 1500: loss 9.536693\n",
      "iteration 700 / 1500: loss 3.997610\n",
      "iteration 800 / 1500: loss 2.377202\n",
      "iteration 900 / 1500: loss 1.344940\n",
      "iteration 1000 / 1500: loss 1.294676\n",
      "iteration 1100 / 1500: loss 1.358060\n",
      "iteration 1200 / 1500: loss 0.642692\n",
      "iteration 1300 / 1500: loss 1.336304\n",
      "iteration 1400 / 1500: loss 1.349339\n",
      "training accuracy: 0.639155\n",
      "validation accuracy: 0.474820\n"
     ]
    }
   ],
   "source": [
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4, num_iters=1500, verbose=True)\n",
    "\n",
    "y_train_pred = svmd.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred),))\n",
    "\n",
    "y_val_pred = svmd.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "An object of class LinearSVM is trained over X_train for 1500 epochs using some hyperparameters, learning rate and regularization strength. These hyperparameters will be tuned to achieve best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters to get the Best Fit\n",
    "\n",
    "#### Use the validation set to the hyperarameters (Regularizition strength and learning rate. You should experiment with different ranges for the learning rates and regularization strenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3]\n",
    "regularization_strengths = [2.5e4, 5e4, 2.5e3, 2.5e2, 5e2, 1e1]\n",
    "\n",
    "results = {}\n",
    "best_val = -1  # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None  # The LinearSVM object that achieved the highest validation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare blr, brg to store the best LinearSVM object's learning rate and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 5.000000e-07 reg 1.000000e+01 train accuracy: 0.995202 val accuracy: 0.448441\n",
      "lr 5.000000e-07 reg 2.500000e+02 train accuracy: 0.995202 val accuracy: 0.513189\n",
      "lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.755758 val accuracy: 0.455635\n",
      "lr 5.000000e-07 reg 2.500000e+03 train accuracy: 0.727927 val accuracy: 0.520384\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.532150 val accuracy: 0.541966\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.512956 val accuracy: 0.455635\n",
      "lr 5.000000e-06 reg 1.000000e+01 train accuracy: 0.995202 val accuracy: 0.443645\n",
      "lr 5.000000e-06 reg 2.500000e+02 train accuracy: 0.668906 val accuracy: 0.520384\n",
      "lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.639635 val accuracy: 0.544365\n",
      "lr 5.000000e-06 reg 2.500000e+03 train accuracy: 0.515355 val accuracy: 0.462830\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.495202 val accuracy: 0.446043\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.496161 val accuracy: 0.465228\n",
      "lr 5.000000e-05 reg 1.000000e+01 train accuracy: 0.714491 val accuracy: 0.450839\n",
      "lr 5.000000e-05 reg 2.500000e+02 train accuracy: 0.519194 val accuracy: 0.467626\n",
      "lr 5.000000e-05 reg 5.000000e+02 train accuracy: 0.514395 val accuracy: 0.544365\n",
      "lr 5.000000e-05 reg 2.500000e+03 train accuracy: 0.495681 val accuracy: 0.460432\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 1.000000e+01 train accuracy: 0.572937 val accuracy: 0.465228\n",
      "lr 5.000000e-04 reg 2.500000e+02 train accuracy: 0.504319 val accuracy: 0.455635\n",
      "lr 5.000000e-04 reg 5.000000e+02 train accuracy: 0.508637 val accuracy: 0.532374\n",
      "lr 5.000000e-04 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-04 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 1.000000e+01 train accuracy: 0.519194 val accuracy: 0.537170\n",
      "lr 5.000000e-03 reg 2.500000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+02 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+03 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.484645 val accuracy: 0.549161\n",
      "best validation accuracy achieved during cross-validation: 0.551559\n",
      "linear SVM on raw pixels final test set accuracy: 0.507177\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-00fa6c25afb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msvmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m loss_hist = svmd.train(X_train, y_train, learning_rate=blr, reg=brg,\n\u001b[0;32m---> 38\u001b[0;31m                        num_iters=2000, verbose=False)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0my_svmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVMD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ae83af230d0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# evaluate loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fdb1e94f0484>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X_batch, y_batch, reg)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msvm_loss_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-568911df0c8c>\u001b[0m in \u001b[0;36msvm_loss_vectorized\u001b[0;34m(W, X, y, reg)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Add regularization to the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmargin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmargin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "blr = None\n",
    "brg = None\n",
    "\n",
    "grid_search = [(lr, rg) for lr in learning_rates for rg in regularization_strengths]\n",
    "for lr, rg in grid_search:\n",
    "    svmd = LinearSVM()\n",
    "    train_loss = svmd.train(X_train, y_train, learning_rate=lr, reg=rg,\n",
    "                            num_iters=2000, verbose=False)\n",
    "    # Predict values for training set\n",
    "    y_train_pred = svmd.predict(X_train)\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = np.mean(y_train_pred == y_train)\n",
    "    # Predict values for validation set\n",
    "    y_val_pred = svmd.predict(X_val)\n",
    "    # Calculate accuracy\n",
    "    val_accuracy = np.mean(y_val_pred == y_val)\n",
    "    # Save results\n",
    "    results[(lr, rg)] = (train_accuracy, val_accuracy)\n",
    "    if best_val < val_accuracy:\n",
    "        blr = lr\n",
    "        brg = rg\n",
    "        best_val = val_accuracy\n",
    "        best_svm = svmd\n",
    "        \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "        lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)\n",
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=blr, reg=brg,\n",
    "                       num_iters=2000, verbose=False)\n",
    "y_svmd = svmd.predict(X_test)\n",
    "print(\"SVMD\")\n",
    "print(y_svmd[0:10])\n",
    "print(\"actual\")\n",
    "print(y_test[0:10])\n",
    "print(\"best_svm\")\n",
    "print(y_test_pred[0:10])\n",
    "import os\n",
    "\n",
    "pred = []\n",
    "svmd = LinearSVM()\n",
    "loss_hist = svmd.train(X_train, y_train, learning_rate=blr, reg=brg,\n",
    "                       num_iters=2000, verbose=False)\n",
    "svmdpred = []\n",
    "ccat = 0\n",
    "cdog = 0\n",
    "for i in range(2550, 2560):\n",
    "    file = os.path.join(\n",
    "        'small_dataset_train/train/',\n",
    "        str(i) + \".jpg\")\n",
    "    img = cv.imread(file)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img_pred = cv.resize(img, (50, 50), interpolation=cv.INTER_AREA)\n",
    "    img_pred = image.img_to_array(img_pred)\n",
    "    img_pred = img_pred / 255\n",
    "    img_pred = np.reshape(img_pred, (1, 3 * img_pred.shape[0] * img_pred.shape[1]))\n",
    "    img_pred -= mean_image\n",
    "    img_pred = np.hstack([img_pred, np.ones((img_pred.shape[0], 1))])\n",
    "    res = svmd.predict(img_pred)\n",
    "    if res[0] == 0:\n",
    "        ccat += 1\n",
    "    else:\n",
    "        cdog += 1\n",
    "    svmdpred.append(res[0])\n",
    "    \n",
    "print(\"Flowers testset with custom implementation: plants\")\n",
    "print(ccat, cdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "classifier = svm.LinearSVC(penalty='l2', loss='squared_hinge', max_iter=2000)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.score(X_train, y_train))\n",
    "print(classifier.score(X_val, y_val))\n",
    "print(classifier.score(X_test, y_test))\n",
    "predx = []\n",
    "ccat = cdog = 0\n",
    "for i in range(1, 50):\n",
    "    file = os.path.join(\n",
    "        'small_datasets_train/train',\n",
    "        str(i) + \".jpg\")\n",
    "    img = cv.imread(file)\n",
    "    # img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img_pred = cv.resize(img, (50, 50), interpolation=cv.INTER_AREA)\n",
    "    img_pred = image.img_to_array(img_pred)\n",
    "    img_pred = img_pred / 255\n",
    "    img_pred = np.reshape(img_pred, (1, 3 * img_pred.shape[0] * img_pred.shape[1]))\n",
    "    img_pred-=mean_image\n",
    "    img_pred = np.hstack([img_pred, np.ones((img_pred.shape[0], 1))])\n",
    "    res = classifier.predict(img_pred)\n",
    "    if res[0] == 0:\n",
    "        ccat += 1\n",
    "    else:\n",
    "        cdog += 1\n",
    "    predx.append(res[0])\n",
    "print(\"Using sklearn lib: Plants\")\n",
    "print(ccat, cdog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
